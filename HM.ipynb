{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74375c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertModel\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertEmbeddings, BertEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee185f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_inputs = torch.load('cls_train_text_inputs.pth')\n",
    "train_text_masks = torch.load('cls_train_text_masks.pth')\n",
    "train_text_attribute_label = torch.load('train_text_attribute_label.pth')\n",
    "train_text_word_together = torch.load('train_text_word_together.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_obj_feat = torch.load('train_img_obj_feat.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948aa1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_obj_bbox = torch.load('train_img_obj_bbox.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb11c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_ocr_feat = torch.load('train_img_ocr_feat.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe088791",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_ocr_bbox = torch.load('train_img_ocr_bbox.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865661f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_masks = torch.load('cls_train_img_masks.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e61e3592",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edit_distance = torch.load('train_edit_distance.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7f5f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text_inputs = torch.load('cls_val_text_inputs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca6f516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text_masks = torch.load('cls_val_text_masks.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3cee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text_attribute_label = torch.load('val_text_attribute_label.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92a5b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text_word_together = torch.load('val_text_word_together.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4bf688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_obj_feat = torch.load('val_img_obj_feat.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a3eb04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_obj_bbox = torch.load('val_img_obj_bbox.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9307d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_ocr_feat = torch.load('val_img_ocr_feat.pth')\n",
    "val_img_ocr_bbox = torch.load('val_img_ocr_bbox.pth')\n",
    "val_img_masks = torch.load('cls_val_img_masks.pth')\n",
    "val_edit_distance = torch.load('val_edit_distance.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74ea6797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为连着的5个都是正样本，所以我们要把正样本分开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adce037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上Dataloader\n",
    "batch_size = 128\n",
    "\n",
    "train_data = TensorDataset(train_text_inputs, train_text_masks, train_text_attribute_label, train_text_word_together,\n",
    "                           train_img_obj_feat, train_img_obj_bbox, train_img_ocr_feat, train_img_ocr_bbox, train_img_masks, train_edit_distance)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3561dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = TensorDataset(val_text_inputs, val_text_masks, val_text_attribute_label, val_text_word_together,\n",
    "                         val_img_obj_feat, val_img_obj_bbox, val_img_ocr_feat, val_img_ocr_bbox, val_img_masks, val_edit_distance)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81bb0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "feda317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OBJ_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(OBJ_Encoder, self).__init__()\n",
    "\n",
    "        self.num_obj_cdn_in = 2048  # 2048-dim\n",
    "        self.num_obj_bbox_in = 4  # 4-dim\n",
    "        self.num_obj_out = 768  # 768-dim\n",
    "        self._build_obj_encoding()\n",
    "\n",
    "    def _build_obj_encoding(self):\n",
    "\n",
    "        # cdn feature\n",
    "        self.cdn_to_mmt_in = nn.Linear(\n",
    "            self.num_obj_cdn_in, self.num_obj_out\n",
    "        )\n",
    "\n",
    "        # OBJ location feature: bounding box coordinates (4-dim)\n",
    "        self.linear_obj_bbox_to_mmt_in = nn.Linear(\n",
    "            self.num_obj_bbox_in, self.num_obj_out\n",
    "        )\n",
    "\n",
    "        self.obj_layer_norm = nn.LayerNorm(self.num_obj_out)\n",
    "        self.obj_bbox_layer_norm = nn.LayerNorm(self.num_obj_out)\n",
    "\n",
    "    def forward(self, obj_cdistnet_in, obj_bbox_in):\n",
    "\n",
    "        obj_feat = self._forward_obj_encoding(\n",
    "            obj_cdistnet_in, obj_bbox_in)\n",
    "\n",
    "        return obj_feat\n",
    "\n",
    "    def _forward_obj_encoding(self, obj_cdistnet_in, obj_bbox_in):\n",
    "\n",
    "        # OBJ appearance feature: cdistnet\n",
    "        obj_cdn = F.normalize(obj_cdistnet_in, dim=-1)\n",
    "\n",
    "        # MLP + LN\n",
    "        cdn_feat = self.obj_layer_norm(\n",
    "            self.cdn_to_mmt_in(obj_cdn))\n",
    "\n",
    "        obj_bbox = obj_bbox_in\n",
    "\n",
    "        bbox_feat = self.obj_bbox_layer_norm(\n",
    "            self.linear_obj_bbox_to_mmt_in(obj_bbox))\n",
    "\n",
    "        # obj_feat\n",
    "        obj_mmt_in = (\n",
    "            cdn_feat + bbox_feat\n",
    "\n",
    "        )\n",
    "        return obj_mmt_in\n",
    "\n",
    "\n",
    "class OCR_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(OCR_Encoder, self).__init__()\n",
    "\n",
    "        self.num_ocr_cdn_in = 2048  # 2048-dim\n",
    "        self.num_ocr_bbox_in = 4  # 4-dim\n",
    "        self.num_ocr_out = 768  # 768-dim\n",
    "        self._build_ocr_encoding()\n",
    "\n",
    "    def _build_ocr_encoding(self):\n",
    "\n",
    "        # ft_concat_cdn feature\n",
    "        self.cdn_to_mmt_in = nn.Linear(\n",
    "            self.num_ocr_cdn_in, self.num_ocr_out\n",
    "        )\n",
    "\n",
    "        # OCR location feature: bounding box coordinates (4-dim)\n",
    "        self.linear_ocr_bbox_to_mmt_in = nn.Linear(\n",
    "            self.num_ocr_bbox_in, self.num_ocr_out\n",
    "        )\n",
    "\n",
    "        self.ocr_layer_norm = nn.LayerNorm(self.num_ocr_out)\n",
    "        self.ocr_bbox_layer_norm = nn.LayerNorm(self.num_ocr_out)\n",
    "\n",
    "    def forward(self, ocr_cdistnet_in, ocr_bbox_in):\n",
    "\n",
    "        ocr_feat = self._forward_ocr_encoding(\n",
    "            ocr_cdistnet_in, ocr_bbox_in)\n",
    "\n",
    "        return ocr_feat\n",
    "\n",
    "    def _forward_ocr_encoding(self, ocr_cdistnet_in, ocr_bbox_in):\n",
    "\n",
    "        # OCR appearance feature: cdistnet\n",
    "        ocr_cdn = F.normalize(ocr_cdistnet_in, dim=-1)\n",
    "\n",
    "        # MLP + LN\n",
    "        cdn_feat = self.ocr_layer_norm(\n",
    "            self.cdn_to_mmt_in(ocr_cdn))\n",
    "\n",
    "        ocr_bbox = ocr_bbox_in\n",
    "\n",
    "        bbox_feat = self.ocr_bbox_layer_norm(\n",
    "            self.linear_ocr_bbox_to_mmt_in(ocr_bbox))\n",
    "\n",
    "        # ocr_feat\n",
    "        ocr_mmt_in = (\n",
    "            cdn_feat + bbox_feat\n",
    "        )\n",
    "        return ocr_mmt_in\n",
    "\n",
    "\n",
    "class CrossModel(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(CrossModel, self).__init__()\n",
    "        # self.config = config\n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        # 上面的Encoder用于visual，文本的我们就用预训练好的\n",
    "        # 实体化Bert模型\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # img cls linear\n",
    "        self.img_cls_Linear =  nn.Linear(768, 768)\n",
    "        # img obj Encoder\n",
    "        self.obj_encoder = OBJ_Encoder()\n",
    "        # img ocr Encoder\n",
    "        self.ocr_encoder = OCR_Encoder()\n",
    "        \"\"\"\n",
    "        # output L\n",
    "        # text L\n",
    "        self.classifier1 = nn.Sequential(\n",
    "            nn.Linear(768, 500),  # 全连接\n",
    "            nn.Dropout(0.2),  # drop 50% neurons\n",
    "            nn.ReLU(),  # 激活函数\n",
    "            nn.Linear(500, 100),  # 全连接\n",
    "        )  # 这里是2层的全连接\n",
    "        # img L\n",
    "        self.classifier2 = nn.Sequential(\n",
    "            nn.Linear(768, 500),  # 全连接\n",
    "            nn.Dropout(0.2),  # drop 50% neurons\n",
    "            nn.ReLU(),  # 激活函数\n",
    "            nn.Linear(500, 100),  # 全连接\n",
    "        )  # 这里是2层的全连接\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "    def forward(self, text_inputs, text_masks, b_img_obj_feat, b_img_obj_bbox, b_img_ocr_feat, b_img_ocr_bbox, img_masks):\n",
    "        # 文本用bert\n",
    "        text_outputs = self.bert(\n",
    "            input_ids=text_inputs, attention_mask=text_masks)  # 这是现有的\n",
    "        # 文本特征最后隐层输出\n",
    "        text_cls = text_outputs[0][:,0,:]\n",
    "        text_last_hidden_state = text_outputs[0][:,1:,:]\n",
    "        \n",
    "        # img_obj\n",
    "        img_obj = self.obj_encoder(b_img_obj_feat, b_img_obj_bbox)\n",
    "\n",
    "        # img_ocr\n",
    "        img_ocr = self.ocr_encoder(b_img_ocr_feat, b_img_ocr_bbox)\n",
    "        \n",
    "        # img_cls\n",
    "        length_img = len(img_ocr)\n",
    "        cls_token = torch.ones((length_img,1,768),dtype=torch.float32,requires_grad=True).to(device)\n",
    "        img_cls_token = F.normalize(self.img_cls_Linear(cls_token))\n",
    "\n",
    "        # 图片用transformer的encoder\n",
    "        img_inputs = torch.cat((img_cls_token, img_obj, img_ocr), dim=1)  # 连接obj和ocr输入特征 obj:36 ocr:10\n",
    "        # 把mask转换一下\n",
    "        img_masks = img_masks[:, None, None, :]\n",
    "        img_masks = (1.0 - img_masks) * -10000.0\n",
    "        img_outputs = self.encoder(img_inputs, attention_mask=img_masks)\n",
    "        # 图像特征最后隐层输出\n",
    "        img_cls = img_outputs['last_hidden_state'][:,0,:]\n",
    "        img_last_hidden_state = img_outputs['last_hidden_state'][:,1:,:]\n",
    "        \n",
    "\n",
    "        return text_cls, text_last_hidden_state, img_cls, img_last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98769d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epochs=2):\n",
    "    \"\"\"\n",
    "    初始化，优化器还有学习率，epochs就是训练次数\n",
    "    \"\"\"\n",
    "    # 初始化我们的Bert分类器\n",
    "    cross_model = CrossModel()\n",
    "    # 用GPU运算\n",
    "    cross_model.to(device)\n",
    "    # 创建优化器\n",
    "\n",
    "    optimizer = AdamW(cross_model.parameters(),\n",
    "                       lr=5e-5,  # 默认学习率\n",
    "                       weight_decay=5e-4,\n",
    "                       eps=1e-7  # 默认精度\n",
    "                       )\n",
    "\n",
    "    \n",
    "    # 训练的总步数\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    # 学习率调度器，说白了就是自适应学习速率\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0,  # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return cross_model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f9b726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class step1_Loss(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(step1_Loss, self).__init__()\n",
    "    \n",
    "    def forward(self, text_cls, img_cls):\n",
    "    # 前者是文本内容的最后隐层信息，后者是图片内容的最后隐层信息\n",
    "\n",
    "        cos_sim = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "        positive_score_list = cos_sim(text_cls, img_cls)\n",
    "\n",
    "        ######################################################\n",
    "        positive_score = positive_score_list[0]\n",
    "        positive_score = torch.exp(positive_score).to(device)\n",
    "\n",
    "        negative_img_score_list = cos_sim(text_cls[0], img_cls) # (negtive+positive)(img no match)\n",
    "        negative_text_score_list = cos_sim(img_cls[0], text_cls) # (negtive+positive)(text no match)\n",
    "        negative_score = torch.exp(negative_img_score_list).sum()+torch.exp(negative_text_score_list).sum()\n",
    "\n",
    "        step1_NCE_loss =  - (torch.log(positive_score/negative_score))\n",
    "        \n",
    "        for i in range(1, len(positive_score_list)):\n",
    "            # 对于每个正样本，都有2*bz-2个负样本\n",
    "            positive_score = positive_score_list[i]\n",
    "            positive_score = torch.exp(positive_score).to(device)\n",
    "\n",
    "            negative_img_score_list = cos_sim(text_cls[i], img_cls) # (negtive+positive)(img no match)\n",
    "            negative_text_score_list = cos_sim(img_cls[i], text_cls) # (negtive+positive)(text no match)\n",
    "            negative_score = torch.exp(negative_img_score_list).sum()+torch.exp(negative_text_score_list).sum()\n",
    "\n",
    "            step1_NCE_loss = step1_NCE_loss - (torch.log(positive_score/negative_score))\n",
    "            \n",
    "         #-----------------------------------------Step 1-----------------------------------------------\n",
    "        return step1_NCE_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cd163cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class step2and3_Loss(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(step2and3_Loss, self).__init__()\n",
    "        self.step1_loss = step1_Loss()\n",
    "    def forward(self, text_last_hidden_state, img_last_hidden_state,b_text_masks,b_img_masks,b_text_attribute_label, b_text_word_together, b_editor_distant, step2=False,step3=False):\n",
    "        \n",
    "        # 前者是文本内容的最后隐层信息，后者是图片内容的最后隐层信息\n",
    "            # 但是他们的维度都是被padding过的，所以我们要根据b_text_masks截取有用的部分作平均池化     \n",
    "         #-----------------------------------------Step 2-----------------------------------------------\n",
    "        if step2:\n",
    "            # step1是全局平均池化，求的是全局的信息，step2则是通过全局粗粒度对齐后，增加的细粒度信息对齐，分别为obj和ocr\n",
    "            # obj通过全局对齐后的逻辑直接找max，然后通过fasttext的类别特征rerank来监督\n",
    "            # obj的监督还是可以用NCE loss\n",
    "            # ocr就是通过CDist识别出来的字符，直接和文本中认为是OCR的单词来作编辑距离监督\n",
    "\n",
    "            #### 首先是obj的 ####\n",
    "            # global训练完了，先直接找最大匹配的token，但是最开始我们还要把text中分词的token合并\n",
    "            # text_last_hidden_state, b_text_word_together, b_text_attribute_label, b_text_masks\n",
    "            # attribute_label中0是不管的，1是obj，2是ocr，3是relation\n",
    "            # 从上面四个输入得到我们要的obj token\n",
    "            # 先用mask来把last_hiddeen_state给截断，得到有意义的部分\n",
    "\n",
    "            # loss obj和ocr\n",
    "            step2_obj_loss = torch.tensor(0,dtype=float).to(device)\n",
    "            step2_ocr_loss = torch.tensor(0,dtype=float).to(device)\n",
    "            step3_relation_loss = torch.tensor(0,dtype=float).to(device)\n",
    "            step2_obj_loss.requires_grad_(True)\n",
    "            step2_ocr_loss.requires_grad_(True)\n",
    "            step3_relation_loss.requires_grad_(True)\n",
    "\n",
    "            cos_sim = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "\n",
    "            for i in range(len(text_last_hidden_state)):\n",
    "                # relation_order来存最大可能的物体，包含obj和ocr\n",
    "                relation_order = []\n",
    "                this_text_last_hidden_state = text_last_hidden_state[i][:(b_text_masks[i].sum()-2)]\n",
    "                # 现在this就是去掩码的有用信息\n",
    "                length = 0\n",
    "                for j in b_text_word_together[i]:\n",
    "                    if j!=-1:\n",
    "                        length+=1\n",
    "                    else:\n",
    "                        break\n",
    "                # length就是实际句子单词的长度\n",
    "                # 然后我们进行合并\n",
    "                this_text_word_together = b_text_word_together[i][:length]\n",
    "                this_text_attribute_label = b_text_attribute_label[i][:length]\n",
    "                counts = 0\n",
    "                for k in range(length):\n",
    "                    # 这里的this_text_word_together[k]就是后面几个token为一个实际单词\n",
    "                    word_len = this_text_word_together[k]\n",
    "                    # ocr_times来记录这是第几个ocr，来返回对应所需要的编辑距离label值\n",
    "                    ocr_times = 0\n",
    "\n",
    "                    # 这是一个单词的最后隐层信息\n",
    "                    if word_len>1:\n",
    "                        word_last_hidden_state = this_text_last_hidden_state[counts:counts+word_len].mean(dim=0)\n",
    "                    else:\n",
    "                        word_last_hidden_state = this_text_last_hidden_state[counts:counts+word_len]\n",
    "\n",
    "                    counts += word_len\n",
    "\n",
    "                    if this_text_attribute_label[k]==1:\n",
    "\n",
    "                        # temp_obj.append(word_last_hidden_state)\n",
    "                        # 这是obj的隐层信息，直接作余弦相似度找最大\n",
    "                        obj_cos_sim = cos_sim(word_last_hidden_state, img_last_hidden_state[i][:36])\n",
    "                        max_obj_i_sim = obj_cos_sim[obj_cos_sim.argmax()]\n",
    "\n",
    "                        # obj_cos_sim.argmax()为该text中obj匹配到最佳img obj的序号\n",
    "                        relation_order.append(int(obj_cos_sim.argmax()))\n",
    "\n",
    "                        #############################   step2_obj_loss，让obj匹配最大的最大     ###############################\n",
    "                        step2_obj_loss = step2_obj_loss - torch.log(torch.exp(max_obj_i_sim-torch.tensor(1).to(device)))\n",
    "\n",
    "\n",
    "                    elif this_text_attribute_label[k]==2:\n",
    "\n",
    "                        # 防止图片中没识别出ocr报错\n",
    "                        if b_img_masks[i].sum()!=37:\n",
    "                            # 这里我们直接得到这句话所有ocr对应的编辑距离\n",
    "                            # b_editor_distant应该被补0了，所以是[bz, 30, 10]的维度\n",
    "                            # 这里的k就是text中ocr的个数\n",
    "                            this_editor_distant = b_editor_distant[i][ocr_times]\n",
    "\n",
    "                            # 图片中有ocr，直接求loss\n",
    "                            ocr_cos_sim = cos_sim(word_last_hidden_state, img_last_hidden_state[i][36:(b_img_masks[i].sum()-1)])\n",
    "                            relation_order.append(int(ocr_cos_sim.argmax() + 36))  \n",
    "                            # 求出来了对应的余弦相似性，用编辑距离作监督\n",
    "                            # 因为要保证差是整数，所以平方即可\n",
    "                            for l in range(len(ocr_cos_sim)):\n",
    "                                #############################   step2_ocr_loss，让ocr匹配与编辑距离最符  ############################\n",
    "                                step2_ocr_loss = step2_ocr_loss + torch.pow((this_editor_distant[l]-ocr_cos_sim[l]),torch.tensor(2).to(device))\n",
    "\n",
    "                        else:\n",
    "                            # 这里就是文本里有识别的ocr，但是图片里面没有，那么我们就把ocr退化为obj，使用求obj_loss的方法\n",
    "                            ocr_cos_sim = cos_sim(word_last_hidden_state, img_last_hidden_state[i][:36])\n",
    "                            max_ocr_i_sim = ocr_cos_sim[ocr_cos_sim.argmax()]\n",
    "                            relation_order.append(int(ocr_cos_sim.argmax())) \n",
    "\n",
    "                            #############################   退化后的ocr loss，让ocr匹配最大的最大     ###############################\n",
    "                            step2_ocr_loss = step2_ocr_loss - torch.log(torch.exp(max_ocr_i_sim-torch.tensor(1).to(device)))\n",
    "\n",
    "                        ocr_times +=1 # ocr_times记录+1\n",
    "                    #-----------------------------------------Step 2-----------------------------------------------\n",
    "                    \n",
    "                counts = 0\n",
    "                relation_order = list(set(relation_order))        \n",
    "                # 取relation对，不会重复取相同元素\n",
    "                if len(relation_order)>1:\n",
    "                    relation_order = list(itertools.combinations(relation_order, 2))\n",
    "                else:\n",
    "                    relation_order = [(relation_order[0], relation_order[0])]\n",
    "\n",
    "                for k in range(length):\n",
    "                    # 这里的this_text_word_together[k]就是后面几个token为一个实际单词\n",
    "                    word_len = this_text_word_together[k]\n",
    "                    # ocr_times来记录这是第几个ocr，来返回对应所需要的编辑距离label值\n",
    "\n",
    "                    # 这是一个单词的最后隐层信息\n",
    "                    if word_len>1:\n",
    "                        word_last_hidden_state = this_text_last_hidden_state[counts:counts+word_len].mean(dim=0)\n",
    "                    else:\n",
    "                        word_last_hidden_state = this_text_last_hidden_state[counts:counts+word_len]\n",
    "\n",
    "                    counts += word_len\n",
    "                    \n",
    "                    if this_text_attribute_label[k]==3:\n",
    "                        #-----------------------------------------Step 3-----------------------------------------------\n",
    "                        if step3:\n",
    "                            # 这个时候我们就要从relation_order里面找最佳匹配的两个物体\n",
    "                            # order直接从last_img_hidden_state[i]里对应\n",
    "                            # 避免重复，先去重序号\n",
    "               \n",
    "                            # max_relation用来记录最大的relation对是谁\n",
    "                            #print(relation_order[0][0])\n",
    "                            #print(relation_order[0][1])\n",
    "                           \n",
    "                            max_relation = cos_sim(word_last_hidden_state,(img_last_hidden_state[i][relation_order[0][0]] + img_last_hidden_state[i][relation_order[0][1]]))\n",
    "\n",
    "                            for l in relation_order:\n",
    "                                # 把relation对特征直接相加，然后和relation text特征作余弦相似度\n",
    "                                relation_pair = img_last_hidden_state[i][l[0]] + img_last_hidden_state[i][l[1]]\n",
    "                                relation_cos_sim = cos_sim(word_last_hidden_state, relation_pair)\n",
    "                              \n",
    "                                if relation_cos_sim>max_relation:\n",
    "                                    max_relation = relation_cos_sim\n",
    "                            # 找到了最大的relation对——max_relation就可以返回loss了\n",
    "                            step3_relation_loss = step3_relation_loss - torch.log(torch.exp(max_relation-torch.tensor(1).to(device)))\n",
    "                        #-----------------------------------------Step 3-----------------------------------------------\n",
    "        if step2==True and step3==False:\n",
    "            return step2_obj_loss + step2_ocr_loss\n",
    "        elif step2==True and step3==True:\n",
    "            return step2_obj_loss + step2_ocr_loss + step3_relation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8832ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCE_loss = step1_Loss()\n",
    "step2and3_loss = step2and3_Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1924df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train(model,  train_dataloader, test_dataloader=None, epochs=2, step1 = True, step2 = False, step3=False, evaluation=False):\n",
    "    Train_loss = []\n",
    "    Test_loss = []\n",
    "\n",
    "    # 开始训练循环\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # 表头\n",
    "        print(\n",
    "            f\"{'Epoch':^7} | {'每10个Batch':^10} | {'训练集 Loss':^14} |{'时间':^9}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        # 测量每个epoch经过的时间\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # 在每个epoch开始时重置跟踪变量\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # 把model放到训练模式\n",
    "        model.train()\n",
    "        times_all = 0\n",
    "        \n",
    "        \n",
    "        # 分batch训练\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts += 1\n",
    "            times_all += 1\n",
    "            # 把batch加载到GPU\n",
    "            b_text_inputs, b_text_masks, b_text_attribute_label, b_text_word_together, b_img_obj_feat, b_img_obj_bbox, b_img_ocr_feat, b_img_ocr_bbox, b_img_masks, b_editor_distant = batch\n",
    "            \n",
    "\n",
    "            b_text_inputs = b_text_inputs.to(device)\n",
    "            b_text_masks = b_text_masks.to(device)\n",
    "            b_text_attribute_label = b_text_attribute_label.to(device)\n",
    "            b_text_word_together = b_text_word_together.to(device)\n",
    "            b_img_obj_feat = b_img_obj_feat.to(device)\n",
    "            b_img_obj_bbox = b_img_obj_bbox.to(device)\n",
    "            b_img_ocr_feat = b_img_ocr_feat.to(device)\n",
    "            b_img_ocr_bbox = b_img_ocr_bbox.to(device)\n",
    "            b_img_masks = b_img_masks.to(device)\n",
    "            b_editor_distant = b_editor_distant.to(device)\n",
    "            \n",
    "            \n",
    "            # 归零导数\n",
    "            model.zero_grad()\n",
    "\n",
    "            # 真正的训练\n",
    "            text_cls, text_last_hidden_state, img_cls, img_last_hidden_state = model(b_text_inputs, b_text_masks, b_img_obj_feat, b_img_obj_bbox, b_img_ocr_feat, b_img_ocr_bbox, b_img_masks)\n",
    "            \n",
    "            \n",
    "            if step1==True and step2==False:\n",
    "                loss = NCE_loss(text_cls,img_cls)\n",
    "                loss.backward()\n",
    "            elif step1==True and step2==True and step3==False:\n",
    "                loss = NCE_loss(text_cls,img_cls) + step2and3_loss(text_last_hidden_state,img_last_hidden_state, b_text_masks, b_img_masks, b_text_attribute_label, b_text_word_together, b_editor_distant,step2=True,step3=False)\n",
    "                loss.backward()\n",
    "            elif step1 == True and step2 == True and step3 == True:\n",
    "                loss = NCE_loss(text_cls,img_clss) + step2and3_loss(text_last_hidden_state,img_last_hidden_state, b_text_masks, b_img_masks, b_text_attribute_label, b_text_word_together, b_editor_distant, step2=True,step3=True)\n",
    "                loss.backward()\n",
    "            \n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            \n",
    "            # 归一化，防止梯度爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # 更新参数和学习率\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            \n",
    "\n",
    "            # Print每10个batch的loss和time\n",
    "            if (step % 10 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # 计算10个batch的时间\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                # Print训练结果\n",
    "                print(\n",
    "                    f\"{epoch_i + 1:^7} | {step:^11} | {batch_loss /batch_counts:^16.6f}| {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # 重置batch参数\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "        \n",
    "        # 每5个存一下模型\n",
    "        save_name = 'cls_cross_model_'+ str(epoch_i+1)+'.pth'\n",
    "        torch.save(model, save_name)\n",
    "            \n",
    "        # 计算平均loss 这个是训练集的loss\n",
    "        avg_train_loss = total_loss / times_all\n",
    "        print(\n",
    "            f\"{epoch_i + 1:^7} | {'-':^10} | {avg_train_loss:^14.6f} | {time_elapsed:^9.2f}\")\n",
    "        print(\"-\" * 100)\n",
    "        print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22908c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/envs/CDistNet/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training and testing:\n",
      "\n",
      " Epoch  | 每10个Batch  |    训练集 Loss    |   时间    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "   1    |     10      |    690.975297   |   10.28  \n",
      "   1    |     20      |    664.385724   |   9.11   \n",
      "   1    |     30      |    657.349463   |   9.10   \n",
      "   1    |     40      |    653.539282   |   9.11   \n",
      "   1    |     50      |    647.840979   |   9.11   \n",
      "   1    |     60      |    644.965833   |   9.11   \n",
      "   1    |     70      |    642.498712   |   9.11   \n",
      "   1    |     80      |    640.474963   |   9.12   \n",
      "   1    |     90      |    637.166901   |   9.11   \n",
      "   1    |     100     |    634.881848   |   9.11   \n",
      "   1    |     110     |    633.286713   |   9.11   \n",
      "   1    |     120     |    632.582648   |   9.12   \n",
      "   1    |     130     |    629.492310   |   9.12   \n",
      "   1    |     140     |    628.413318   |   9.11   \n",
      "   1    |     150     |    627.319122   |   9.16   \n",
      "   1    |     160     |    627.222223   |   9.12   \n",
      "   1    |     170     |    625.046313   |   9.11   \n",
      "   1    |     180     |    623.934100   |   9.11   \n",
      "   1    |     190     |    624.705865   |   9.11   \n",
      "   1    |     200     |    624.725360   |   9.12   \n",
      "   1    |     210     |    622.506940   |   9.11   \n",
      "   1    |     220     |    621.883545   |   9.12   \n",
      "   1    |     230     |    622.656342   |   9.11   \n",
      "   1    |     240     |    621.374890   |   9.12   \n",
      "   1    |     250     |    620.621979   |   9.12   \n",
      "   1    |     260     |    620.647687   |   9.12   \n",
      "   1    |     270     |    620.405676   |   9.12   \n",
      "   1    |     280     |    619.506024   |   9.11   \n",
      "   1    |     290     |    619.632306   |   9.12   \n",
      "   1    |     300     |    620.074365   |   9.11   \n",
      "   1    |     310     |    619.438531   |   9.11   \n",
      "   1    |     320     |    621.440198   |   9.12   \n",
      "   1    |     330     |    619.590393   |   9.11   \n",
      "   1    |     340     |    619.599286   |   9.11   \n",
      "   1    |     350     |    618.015704   |   9.12   \n",
      "   1    |     360     |    618.414716   |   9.12   \n",
      "   1    |     370     |    618.261615   |   9.12   \n",
      "   1    |     380     |    618.165765   |   9.12   \n",
      "   1    |     390     |    618.938550   |   9.12   \n",
      "   1    |     400     |    617.818768   |   9.12   \n",
      "   1    |     410     |    618.405402   |   9.13   \n",
      "   1    |     420     |    616.908319   |   9.12   \n",
      "   1    |     430     |    617.689850   |   9.11   \n",
      "   1    |     440     |    618.418536   |   9.12   \n",
      "   1    |     450     |    616.996631   |   9.12   \n",
      "   1    |     460     |    618.219867   |   9.12   \n",
      "   1    |     470     |    617.164313   |   9.17   \n",
      "   1    |     480     |    616.043164   |   9.11   \n",
      "   1    |     490     |    616.141083   |   9.12   \n",
      "   1    |     500     |    614.862897   |   9.11   \n",
      "   1    |     510     |    615.499280   |   9.12   \n",
      "   1    |     520     |    614.771259   |   9.12   \n",
      "   1    |     530     |    615.657568   |   9.12   \n",
      "   1    |     540     |    615.917993   |   9.12   \n",
      "   1    |     550     |    613.005566   |   9.12   \n",
      "   1    |     560     |    614.108807   |   9.12   \n",
      "   1    |     570     |    614.520087   |   9.12   \n",
      "   1    |     580     |    613.699463   |   9.12   \n",
      "   1    |     590     |    613.316913   |   9.12   \n",
      "   1    |     600     |    613.413965   |   9.12   \n",
      "   1    |     610     |    612.670190   |   9.12   \n",
      "   1    |     620     |    612.455481   |   9.12   \n",
      "   1    |     630     |    611.658759   |   9.11   \n",
      "   1    |     640     |    613.486774   |   9.18   \n",
      "   1    |     650     |    615.573999   |   9.12   \n",
      "   1    |     660     |    612.549658   |   9.12   \n",
      "   1    |     670     |    612.085077   |   9.12   \n",
      "   1    |     680     |    612.429254   |   9.13   \n",
      "   1    |     690     |    612.850732   |   9.12   \n",
      "   1    |     700     |    612.759155   |   9.12   \n",
      "   1    |     710     |    611.968567   |   9.12   \n",
      "   1    |     720     |    614.016046   |   9.12   \n",
      "   1    |     730     |    612.848993   |   9.12   \n",
      "   1    |     740     |    613.379999   |   9.13   \n",
      "   1    |     750     |    612.092297   |   9.12   \n",
      "   1    |     760     |    612.193982   |   9.12   \n",
      "   1    |     770     |    610.537341   |   9.12   \n",
      "   1    |     780     |    611.658350   |   9.12   \n",
      "   1    |     790     |    611.738843   |   9.12   \n",
      "   1    |     800     |    612.360950   |   9.13   \n",
      "   1    |     810     |    611.944592   |   9.12   \n",
      "   1    |     820     |    611.301465   |   9.12   \n",
      "   1    |     830     |    610.710187   |   9.12   \n",
      "   1    |     840     |    612.717041   |   9.12   \n",
      "   1    |     850     |    611.495251   |   9.12   \n",
      "   1    |     860     |    612.328094   |   9.12   \n",
      "   1    |     870     |    611.985968   |   9.12   \n",
      "   1    |     880     |    612.322510   |   9.12   \n",
      "   1    |     890     |    612.615192   |   9.12   \n",
      "   1    |     900     |    611.200433   |   9.12   \n",
      "   1    |     910     |    611.588922   |   9.12   \n",
      "   1    |     920     |    611.125427   |   9.12   \n",
      "   1    |     930     |    610.199158   |   9.12   \n",
      "   1    |     940     |    610.607874   |   9.12   \n",
      "   1    |     950     |    609.283484   |   9.13   \n",
      "   1    |     960     |    611.499347   |   9.17   \n",
      "   1    |     970     |    610.944482   |   9.12   \n",
      "   1    |     980     |    610.575714   |   9.13   \n",
      "   1    |     990     |    610.133362   |   9.12   \n",
      "   1    |    1000     |    610.539398   |   9.13   \n",
      "   1    |    1010     |    608.950934   |   9.12   \n",
      "   1    |    1020     |    609.168640   |   9.13   \n",
      "   1    |    1030     |    610.429694   |   9.12   \n",
      "   1    |    1040     |    610.339532   |   9.12   \n",
      "   1    |    1050     |    609.871338   |   9.12   \n",
      "   1    |    1060     |    610.320538   |   9.12   \n",
      "   1    |    1070     |    610.040363   |   9.13   \n",
      "   1    |    1080     |    608.748120   |   9.12   \n",
      "   1    |    1090     |    609.990845   |   9.12   \n",
      "   1    |    1100     |    610.395898   |   9.12   \n",
      "   1    |    1110     |    609.437762   |   9.13   \n",
      "   1    |    1120     |    610.555005   |   9.12   \n",
      "   1    |    1125     |    542.032288   |   4.20   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/CDistNet/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type CrossModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/miniconda3/envs/CDistNet/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type OBJ_Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/root/miniconda3/envs/CDistNet/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type OCR_Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |     -      |   618.466749   |   4.20   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  | 每10个Batch  |    训练集 Loss    |   时间    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "   2    |     10      |    608.794794   |   10.05  \n",
      "   2    |     20      |    607.737152   |   9.12   \n",
      "   2    |     30      |    608.640491   |   9.12   \n",
      "   2    |     40      |    607.426642   |   10.82  \n",
      "   2    |     50      |    607.662024   |   9.13   \n",
      "   2    |     60      |    607.748511   |   9.13   \n",
      "   2    |     70      |    608.312158   |   9.13   \n",
      "   2    |     80      |    607.790393   |   9.13   \n",
      "   2    |     90      |    607.621326   |   9.13   \n",
      "   2    |     100     |    607.434760   |   9.13   \n",
      "   2    |     110     |    607.941754   |   9.13   \n",
      "   2    |     120     |    607.491083   |   9.13   \n",
      "   2    |     130     |    608.093140   |   9.13   \n",
      "   2    |     140     |    607.769812   |   9.13   \n",
      "   2    |     150     |    607.511945   |   9.13   \n",
      "   2    |     160     |    608.034912   |   9.13   \n",
      "   2    |     170     |    607.661047   |   9.13   \n",
      "   2    |     180     |    607.456543   |   9.13   \n",
      "   2    |     190     |    607.954089   |   9.13   \n",
      "   2    |     200     |    607.580133   |   9.13   \n",
      "   2    |     210     |    607.169366   |   9.13   \n",
      "   2    |     220     |    606.643707   |   9.13   \n",
      "   2    |     230     |    607.589941   |   9.13   \n",
      "   2    |     240     |    607.377917   |   9.14   \n",
      "   2    |     250     |    607.100970   |   9.13   \n",
      "   2    |     260     |    606.729938   |   9.13   \n",
      "   2    |     270     |    607.508289   |   9.14   \n",
      "   2    |     280     |    607.465973   |   9.14   \n",
      "   2    |     290     |    608.035938   |   9.14   \n",
      "   2    |     300     |    607.117938   |   9.14   \n",
      "   2    |     310     |    607.002399   |   9.19   \n",
      "   2    |     320     |    607.519305   |   9.12   \n",
      "   2    |     330     |    607.283734   |   9.13   \n",
      "   2    |     340     |    605.805096   |   9.13   \n",
      "   2    |     350     |    607.392297   |   9.13   \n",
      "   2    |     360     |    607.324420   |   9.12   \n",
      "   2    |     370     |    605.980865   |   9.13   \n",
      "   2    |     380     |    607.349567   |   9.13   \n",
      "   2    |     390     |    606.319702   |   9.12   \n",
      "   2    |     400     |    606.664990   |   9.13   \n",
      "   2    |     410     |    606.545850   |   9.12   \n",
      "   2    |     420     |    607.083856   |   9.12   \n",
      "   2    |     430     |    606.447107   |   9.12   \n",
      "   2    |     440     |    607.898474   |   9.13   \n",
      "   2    |     450     |    607.084009   |   9.12   \n",
      "   2    |     460     |    607.520459   |   9.12   \n",
      "   2    |     470     |    606.476031   |   9.12   \n",
      "   2    |     480     |    606.728619   |   9.17   \n",
      "   2    |     490     |    606.770691   |   9.12   \n",
      "   2    |     500     |    606.465070   |   9.12   \n",
      "   2    |     510     |    605.861322   |   9.13   \n",
      "   2    |     520     |    606.684735   |   9.12   \n",
      "   2    |     530     |    606.938416   |   9.13   \n",
      "   2    |     540     |    607.600140   |   9.13   \n",
      "   2    |     550     |    605.122125   |   9.14   \n",
      "   2    |     560     |    607.559277   |   9.13   \n",
      "   2    |     570     |    606.375806   |   9.13   \n",
      "   2    |     580     |    605.564648   |   9.13   \n",
      "   2    |     590     |    606.222418   |   9.12   \n",
      "   2    |     600     |    605.884003   |   9.13   \n",
      "   2    |     610     |    606.621997   |   9.12   \n",
      "   2    |     620     |    605.046912   |   9.13   \n",
      "   2    |     630     |    606.383813   |   9.12   \n",
      "   2    |     640     |    606.152625   |   9.12   \n",
      "   2    |     650     |    607.235815   |   9.12   \n",
      "   2    |     660     |    606.071375   |   9.13   \n",
      "   2    |     670     |    606.442175   |   9.13   \n",
      "   2    |     680     |    605.311383   |   9.12   \n",
      "   2    |     690     |    606.592804   |   9.13   \n",
      "   2    |     700     |    605.599518   |   9.13   \n",
      "   2    |     710     |    606.923132   |   9.13   \n",
      "   2    |     720     |    605.573615   |   9.13   \n",
      "   2    |     730     |    604.377795   |   9.13   \n",
      "   2    |     740     |    606.321216   |   9.12   \n",
      "   2    |     750     |    605.983179   |   9.13   \n",
      "   2    |     760     |    605.245709   |   9.13   \n",
      "   2    |     770     |    607.337512   |   9.12   \n",
      "   2    |     780     |    606.365369   |   9.13   \n",
      "   2    |     790     |    607.152826   |   9.18   \n",
      "   2    |     800     |    605.955609   |   9.12   \n",
      "   2    |     810     |    604.921124   |   9.12   \n",
      "   2    |     820     |    605.663690   |   9.12   \n",
      "   2    |     830     |    605.171777   |   9.12   \n",
      "   2    |     840     |    605.608673   |   9.14   \n",
      "   2    |     850     |    605.916333   |   9.12   \n",
      "   2    |     860     |    606.946344   |   9.16   \n",
      "   2    |     870     |    605.438318   |   9.65   \n",
      "   2    |     880     |    606.026282   |   9.16   \n",
      "   2    |     890     |    606.183508   |   9.11   \n",
      "   2    |     900     |    606.765460   |   9.12   \n",
      "   2    |     910     |    605.540314   |   9.13   \n",
      "   2    |     920     |    605.647827   |   9.99   \n",
      "   2    |     930     |    606.493658   |   9.11   \n",
      "   2    |     940     |    605.436523   |   9.11   \n",
      "   2    |     950     |    605.328851   |   9.11   \n",
      "   2    |     960     |    605.036835   |   9.17   \n",
      "   2    |     970     |    606.754980   |   9.11   \n",
      "   2    |     980     |    606.537537   |   9.12   \n",
      "   2    |     990     |    606.577338   |   9.12   \n",
      "   2    |    1000     |    606.314343   |   9.11   \n",
      "   2    |    1010     |    604.076526   |   9.11   \n",
      "   2    |    1020     |    605.492737   |   9.12   \n",
      "   2    |    1030     |    604.877869   |   9.11   \n",
      "   2    |    1040     |    605.777673   |   10.42  \n",
      "   2    |    1050     |    605.517896   |   9.22   \n",
      "   2    |    1060     |    605.359039   |   9.10   \n",
      "   2    |    1070     |    603.891064   |   9.10   \n",
      "   2    |    1080     |    604.123694   |   9.10   \n",
      "   2    |    1090     |    604.858630   |   9.11   \n",
      "   2    |    1100     |    604.927893   |   9.10   \n",
      "   2    |    1110     |    605.541992   |   9.12   \n",
      "   2    |    1120     |    604.912372   |   9.11   \n",
      "   2    |    1125     |    538.471753   |   4.14   \n",
      "   2    |     -      |   606.228730   |   4.14   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  | 每10个Batch  |    训练集 Loss    |   时间    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "   3    |     10      |    603.238032   |   10.03  \n",
      "   3    |     20      |    603.262439   |   9.12   \n",
      "   3    |     30      |    603.101160   |   9.11   \n",
      "   3    |     40      |    603.080811   |   9.13   \n",
      "   3    |     50      |    602.777087   |   9.11   \n",
      "   3    |     60      |    603.090942   |   9.11   \n",
      "   3    |     70      |    602.805096   |   9.12   \n",
      "   3    |     80      |    602.904218   |   9.12   \n",
      "   3    |     90      |    603.733594   |   9.12   \n",
      "   3    |     100     |    602.451691   |   9.11   \n",
      "   3    |     110     |    602.102252   |   9.11   \n",
      "   3    |     120     |    604.060382   |   9.12   \n",
      "   3    |     130     |    603.955530   |   9.14   \n",
      "   3    |     140     |    603.762384   |   9.12   \n",
      "   3    |     150     |    603.805774   |   9.17   \n",
      "   3    |     160     |    603.879865   |   9.12   \n",
      "   3    |     170     |    602.880420   |   9.12   \n",
      "   3    |     180     |    603.444232   |   9.12   \n",
      "   3    |     190     |    602.626398   |   9.12   \n",
      "   3    |     200     |    603.346936   |   9.12   \n",
      "   3    |     210     |    604.857813   |   9.12   \n",
      "   3    |     220     |    602.999597   |   9.13   \n",
      "   3    |     230     |    603.170599   |   9.12   \n",
      "   3    |     240     |    602.764209   |   9.12   \n",
      "   3    |     250     |    604.079315   |   9.12   \n",
      "   3    |     260     |    602.826385   |   9.13   \n",
      "   3    |     270     |    603.076825   |   9.13   \n",
      "   3    |     280     |    602.682068   |   9.11   \n",
      "   3    |     290     |    603.535132   |   9.12   \n",
      "   3    |     300     |    603.079565   |   9.12   \n",
      "   3    |     310     |    603.684259   |   9.13   \n",
      "   3    |     320     |    602.966083   |   9.17   \n",
      "   3    |     330     |    602.746326   |   9.13   \n",
      "   3    |     340     |    603.085028   |   9.12   \n",
      "   3    |     350     |    603.230621   |   9.12   \n",
      "   3    |     360     |    602.924036   |   9.12   \n",
      "   3    |     370     |    602.813837   |   9.11   \n",
      "   3    |     380     |    601.847882   |   9.12   \n",
      "   3    |     390     |    603.696942   |   9.12   \n",
      "   3    |     400     |    602.395551   |   9.12   \n",
      "   3    |     410     |    602.910016   |   9.11   \n",
      "   3    |     420     |    602.000470   |   9.11   \n",
      "   3    |     430     |    602.850861   |   9.12   \n",
      "   3    |     440     |    601.977740   |   9.12   \n",
      "   3    |     450     |    602.802393   |   9.13   \n",
      "   3    |     460     |    603.679651   |   9.12   \n",
      "   3    |     470     |    603.467090   |   9.12   \n",
      "   3    |     480     |    602.659479   |   9.11   \n",
      "   3    |     490     |    603.220502   |   9.12   \n",
      "   3    |     500     |    602.825134   |   9.11   \n",
      "   3    |     510     |    603.831219   |   9.12   \n",
      "   3    |     520     |    602.789581   |   9.12   \n",
      "   3    |     530     |    603.163904   |   9.12   \n",
      "   3    |     540     |    603.604779   |   9.11   \n",
      "   3    |     550     |    602.754211   |   9.12   \n",
      "   3    |     560     |    603.234717   |   9.12   \n",
      "   3    |     570     |    603.592920   |   9.12   \n",
      "   3    |     580     |    603.322034   |   9.12   \n",
      "   3    |     590     |    603.081995   |   9.11   \n",
      "   3    |     600     |    602.845282   |   9.11   \n",
      "   3    |     610     |    602.637439   |   9.12   \n",
      "   3    |     620     |    603.194952   |   9.12   \n",
      "   3    |     630     |    602.724451   |   9.17   \n",
      "   3    |     640     |    602.327618   |   9.12   \n",
      "   3    |     650     |    603.709607   |   9.13   \n",
      "   3    |     660     |    603.115491   |   9.12   \n",
      "   3    |     670     |    601.926538   |   9.12   \n",
      "   3    |     680     |    602.641699   |   9.11   \n",
      "   3    |     690     |    602.354449   |   9.13   \n",
      "   3    |     700     |    603.012054   |   9.12   \n",
      "   3    |     710     |    602.217682   |   9.12   \n",
      "   3    |     720     |    602.575153   |   9.11   \n",
      "   3    |     730     |    602.294513   |   9.12   \n",
      "   3    |     740     |    601.999011   |   9.12   \n",
      "   3    |     750     |    602.254462   |   9.12   \n",
      "   3    |     760     |    601.946210   |   9.12   \n",
      "   3    |     770     |    602.640668   |   9.12   \n",
      "   3    |     780     |    602.365503   |   9.13   \n",
      "   3    |     790     |    603.042682   |   9.13   \n",
      "   3    |     800     |    602.870605   |   9.18   \n",
      "   3    |     810     |    601.840198   |   9.11   \n",
      "   3    |     820     |    603.531201   |   9.12   \n",
      "   3    |     830     |    602.219934   |   9.12   \n",
      "   3    |     840     |    602.788947   |   9.12   \n",
      "   3    |     850     |    602.382440   |   9.12   \n",
      "   3    |     860     |    601.811432   |   9.12   \n",
      "   3    |     870     |    601.522577   |   9.12   \n",
      "   3    |     880     |    602.215533   |   9.12   \n",
      "   3    |     890     |    602.198492   |   9.12   \n",
      "   3    |     900     |    602.332288   |   9.12   \n",
      "   3    |     910     |    602.693884   |   9.13   \n",
      "   3    |     920     |    602.691595   |   9.12   \n",
      "   3    |     930     |    601.345337   |   9.12   \n",
      "   3    |     940     |    601.762018   |   9.12   \n",
      "   3    |     950     |    601.904205   |   9.12   \n",
      "   3    |     960     |    602.183112   |   9.12   \n",
      "   3    |     970     |    601.757019   |   9.12   \n",
      "   3    |     980     |    602.399536   |   9.12   \n",
      "   3    |     990     |    602.143488   |   9.12   \n",
      "   3    |    1000     |    601.832806   |   9.12   \n",
      "   3    |    1010     |    601.919885   |   9.11   \n",
      "   3    |    1020     |    601.582227   |   9.12   \n",
      "   3    |    1030     |    603.444403   |   9.13   \n",
      "   3    |    1040     |    602.470801   |   9.12   \n",
      "   3    |    1050     |    602.257050   |   9.12   \n",
      "   3    |    1060     |    601.766736   |   9.12   \n",
      "   3    |    1070     |    601.706061   |   9.13   \n",
      "   3    |    1080     |    603.255463   |   9.12   \n",
      "   3    |    1090     |    603.303754   |   9.12   \n",
      "   3    |    1100     |    601.673358   |   9.12   \n",
      "   3    |    1110     |    601.616895   |   9.14   \n",
      "   3    |    1120     |    602.546216   |   9.18   \n",
      "   3    |    1125     |    534.660840   |   4.14   \n",
      "   3    |     -      |   602.468821   |   4.14   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  | 每10个Batch  |    训练集 Loss    |   时间    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "   4    |     10      |    600.613736   |   10.02  \n",
      "   4    |     20      |    601.041772   |   9.12   \n",
      "   4    |     30      |    600.636389   |   9.11   \n",
      "   4    |     40      |    601.296899   |   9.13   \n",
      "   4    |     50      |    600.067365   |   9.11   \n",
      "   4    |     60      |    599.375006   |   9.11   \n",
      "   4    |     70      |    601.196021   |   9.12   \n",
      "   4    |     80      |    601.133978   |   9.12   \n",
      "   4    |     90      |    600.865210   |   9.12   \n",
      "   4    |     100     |    601.598328   |   9.11   \n",
      "   4    |     110     |    601.236273   |   9.12   \n",
      "   4    |     120     |    600.534821   |   9.11   \n",
      "   4    |     130     |    600.337274   |   9.11   \n",
      "   4    |     140     |    600.037036   |   9.11   \n",
      "   4    |     150     |    600.961713   |   9.11   \n",
      "   4    |     160     |    600.327734   |   9.12   \n",
      "   4    |     170     |    600.576050   |   9.11   \n",
      "   4    |     180     |    600.254083   |   9.11   \n",
      "   4    |     190     |    601.122266   |   9.11   \n",
      "   4    |     200     |    600.999390   |   9.11   \n",
      "   4    |     210     |    600.245862   |   9.11   \n",
      "   4    |     220     |    600.728363   |   9.12   \n",
      "   4    |     230     |    600.254956   |   9.11   \n",
      "   4    |     240     |    599.909631   |   9.11   \n",
      "   4    |     250     |    600.299261   |   9.11   \n",
      "   4    |     260     |    600.501465   |   9.11   \n",
      "   4    |     270     |    600.804535   |   9.12   \n",
      "   4    |     280     |    600.207220   |   9.11   \n",
      "   4    |     290     |    600.854755   |   9.12   \n",
      "   4    |     300     |    600.807562   |   9.11   \n",
      "   4    |     310     |    600.123431   |   9.17   \n",
      "   4    |     320     |    600.570050   |   9.12   \n",
      "   4    |     330     |    599.769391   |   9.11   \n",
      "   4    |     340     |    600.322107   |   9.12   \n",
      "   4    |     350     |    600.193524   |   9.13   \n",
      "   4    |     360     |    600.712970   |   9.14   \n",
      "   4    |     370     |    600.558392   |   9.11   \n",
      "   4    |     380     |    600.909875   |   9.12   \n",
      "   4    |     390     |    600.678479   |   9.11   \n",
      "   4    |     400     |    600.756622   |   9.11   \n",
      "   4    |     410     |    600.413239   |   9.11   \n",
      "   4    |     420     |    600.521729   |   9.11   \n",
      "   4    |     430     |    599.806470   |   9.11   \n",
      "   4    |     440     |    600.995758   |   9.11   \n",
      "   4    |     450     |    600.311981   |   9.12   \n",
      "   4    |     460     |    601.670203   |   9.11   \n",
      "   4    |     470     |    600.145862   |   9.12   \n",
      "   4    |     480     |    600.747021   |   9.17   \n",
      "   4    |     490     |    600.207446   |   9.12   \n",
      "   4    |     500     |    600.722632   |   9.12   \n",
      "   4    |     510     |    599.838464   |   9.12   \n",
      "   4    |     520     |    600.857208   |   9.12   \n",
      "   4    |     530     |    600.880914   |   9.11   \n",
      "   4    |     540     |    600.300342   |   9.11   \n",
      "   4    |     550     |    600.762469   |   9.11   \n",
      "   4    |     560     |    600.709100   |   9.11   \n",
      "   4    |     570     |    599.971167   |   9.11   \n",
      "   4    |     580     |    599.758356   |   9.12   \n",
      "   4    |     590     |    600.177448   |   9.12   \n",
      "   4    |     600     |    600.416174   |   9.12   \n",
      "   4    |     610     |    600.791925   |   9.11   \n",
      "   4    |     620     |    600.351160   |   9.12   \n",
      "   4    |     630     |    600.242499   |   9.11   \n",
      "   4    |     640     |    599.999103   |   9.13   \n",
      "   4    |     650     |    600.360876   |   9.12   \n",
      "   4    |     660     |    600.368201   |   9.12   \n",
      "   4    |     670     |    601.238416   |   9.12   \n",
      "   4    |     680     |    600.044867   |   9.12   \n",
      "   4    |     690     |    600.387677   |   9.11   \n",
      "   4    |     700     |    600.631744   |   9.11   \n",
      "   4    |     710     |    599.952478   |   9.12   \n",
      "   4    |     720     |    600.206885   |   9.11   \n",
      "   4    |     730     |    600.286090   |   9.11   \n",
      "   4    |     740     |    600.363080   |   9.12   \n",
      "   4    |     750     |    600.033435   |   9.12   \n",
      "   4    |     760     |    600.663995   |   9.12   \n",
      "   4    |     770     |    600.676556   |   9.12   \n",
      "   4    |     780     |    599.897272   |   9.12   \n",
      "   4    |     790     |    599.590631   |   9.11   \n",
      "   4    |     800     |    600.973065   |   9.17   \n",
      "   4    |     810     |    599.654346   |   9.12   \n",
      "   4    |     820     |    600.704315   |   9.12   \n",
      "   4    |     830     |    599.679388   |   9.12   \n",
      "   4    |     840     |    600.086646   |   9.11   \n",
      "   4    |     850     |    600.226709   |   9.11   \n",
      "   4    |     860     |    601.065015   |   9.11   \n",
      "   4    |     870     |    599.449408   |   9.11   \n",
      "   4    |     880     |    600.902661   |   9.11   \n",
      "   4    |     890     |    600.419611   |   9.11   \n",
      "   4    |     900     |    600.361688   |   9.12   \n",
      "   4    |     910     |    599.584332   |   9.12   \n",
      "   4    |     920     |    599.106244   |   9.11   \n",
      "   4    |     930     |    600.365985   |   9.11   \n",
      "   4    |     940     |    599.778198   |   9.11   \n",
      "   4    |     950     |    600.888702   |   9.11   \n",
      "   4    |     960     |    600.327887   |   9.12   \n",
      "   4    |     970     |    599.877148   |   9.18   \n",
      "   4    |     980     |    599.835785   |   9.13   \n",
      "   4    |     990     |    600.370142   |   9.12   \n",
      "   4    |    1000     |    600.197375   |   9.12   \n",
      "   4    |    1010     |    599.852460   |   9.11   \n",
      "   4    |    1020     |    601.065247   |   9.12   \n",
      "   4    |    1030     |    600.095227   |   9.11   \n",
      "   4    |    1040     |    599.215173   |   9.12   \n",
      "   4    |    1050     |    600.010046   |   9.13   \n",
      "   4    |    1060     |    600.510754   |   9.12   \n",
      "   4    |    1070     |    600.231616   |   9.12   \n",
      "   4    |    1080     |    600.406415   |   9.11   \n",
      "   4    |    1090     |    599.922626   |   9.12   \n",
      "   4    |    1100     |    598.556726   |   9.12   \n",
      "   4    |    1110     |    599.067603   |   9.11   \n",
      "   4    |    1120     |    600.202356   |   9.11   \n",
      "   4    |    1125     |    533.097388   |   4.14   \n",
      "   4    |     -      |   600.073904   |   4.14   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  | 每10个Batch  |    训练集 Loss    |   时间    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "   5    |     10      |    598.797929   |   10.03  \n",
      "   5    |     20      |    599.423608   |   9.11   \n",
      "   5    |     30      |    599.637311   |   9.11   \n",
      "   5    |     40      |    598.341351   |   9.14   \n",
      "   5    |     50      |    598.818024   |   9.12   \n",
      "   5    |     60      |    599.106122   |   9.11   \n",
      "   5    |     70      |    599.516437   |   9.11   \n",
      "   5    |     80      |    599.079248   |   9.11   \n",
      "   5    |     90      |    598.364227   |   9.11   \n",
      "   5    |     100     |    598.291852   |   9.11   \n",
      "   5    |     110     |    598.035834   |   9.12   \n",
      "   5    |     120     |    599.051251   |   9.12   \n",
      "   5    |     130     |    599.228809   |   9.12   \n",
      "   5    |     140     |    598.862866   |   9.11   \n",
      "   5    |     150     |    598.831879   |   9.11   \n",
      "   5    |     160     |    598.640051   |   9.17   \n",
      "   5    |     170     |    599.010376   |   9.11   \n",
      "   5    |     180     |    598.514337   |   9.12   \n",
      "   5    |     190     |    598.203583   |   9.11   \n",
      "   5    |     200     |    599.031012   |   9.12   \n",
      "   5    |     210     |    599.074701   |   9.11   \n",
      "   5    |     220     |    598.817181   |   9.12   \n",
      "   5    |     230     |    598.597015   |   9.12   \n",
      "   5    |     240     |    599.138220   |   9.12   \n",
      "   5    |     250     |    599.556628   |   9.11   \n",
      "   5    |     260     |    599.800787   |   9.11   \n",
      "   5    |     270     |    598.868518   |   9.11   \n",
      "   5    |     280     |    599.088904   |   9.11   \n",
      "   5    |     290     |    599.286963   |   9.12   \n",
      "   5    |     300     |    598.944269   |   9.11   \n",
      "   5    |     310     |    598.756134   |   9.12   \n",
      "   5    |     320     |    598.544324   |   9.12   \n",
      "   5    |     330     |    599.336108   |   9.17   \n",
      "   5    |     340     |    598.801965   |   9.12   \n",
      "   5    |     350     |    598.247876   |   9.11   \n",
      "   5    |     360     |    598.140387   |   9.12   \n",
      "   5    |     370     |    599.001392   |   9.11   \n",
      "   5    |     380     |    598.573248   |   9.12   \n",
      "   5    |     390     |    598.877509   |   9.11   \n",
      "   5    |     400     |    598.875989   |   9.12   \n",
      "   5    |     410     |    598.567767   |   9.11   \n",
      "   5    |     420     |    598.606000   |   9.11   \n",
      "   5    |     430     |    598.802441   |   9.12   \n",
      "   5    |     440     |    598.536078   |   9.12   \n",
      "   5    |     450     |    598.570728   |   9.11   \n",
      "   5    |     460     |    598.993591   |   9.11   \n",
      "   5    |     470     |    598.921771   |   9.11   \n",
      "   5    |     480     |    598.218048   |   9.12   \n",
      "   5    |     490     |    599.224738   |   9.11   \n",
      "   5    |     500     |    599.035370   |   9.12   \n",
      "   5    |     510     |    598.247791   |   9.13   \n",
      "   5    |     520     |    598.897711   |   9.11   \n",
      "   5    |     530     |    598.069141   |   9.11   \n",
      "   5    |     540     |    598.439191   |   9.11   \n",
      "   5    |     550     |    598.828674   |   9.12   \n",
      "   5    |     560     |    599.348969   |   9.12   \n",
      "   5    |     570     |    598.687915   |   9.12   \n",
      "   5    |     580     |    599.214966   |   9.12   \n",
      "   5    |     590     |    598.261108   |   9.11   \n",
      "   5    |     600     |    598.935419   |   9.11   \n",
      "   5    |     610     |    598.658038   |   9.11   \n",
      "   5    |     620     |    598.511945   |   9.12   \n",
      "   5    |     630     |    598.337366   |   9.12   \n",
      "   5    |     640     |    598.833728   |   9.17   \n",
      "   5    |     650     |    598.803534   |   9.11   \n",
      "   5    |     660     |    598.399426   |   9.12   \n",
      "   5    |     670     |    598.454358   |   9.12   \n",
      "   5    |     680     |    598.334351   |   9.11   \n",
      "   5    |     690     |    598.595966   |   9.11   \n",
      "   5    |     700     |    599.025580   |   9.11   \n",
      "   5    |     710     |    598.781873   |   9.11   \n",
      "   5    |     720     |    599.099207   |   9.11   \n",
      "   5    |     730     |    598.915442   |   9.12   \n",
      "   5    |     740     |    598.221967   |   9.12   \n",
      "   5    |     750     |    598.321112   |   9.12   \n",
      "   5    |     760     |    598.862946   |   9.12   \n",
      "   5    |     770     |    598.237244   |   9.11   \n",
      "   5    |     780     |    599.420319   |   9.12   \n",
      "   5    |     790     |    598.720062   |   9.12   \n",
      "   5    |     800     |    597.852972   |   9.12   \n",
      "   5    |     810     |    598.542023   |   9.17   \n",
      "   5    |     820     |    598.947034   |   9.12   \n",
      "   5    |     830     |    598.553162   |   9.12   \n",
      "   5    |     840     |    598.669617   |   9.12   \n",
      "   5    |     850     |    598.792340   |   9.11   \n",
      "   5    |     860     |    598.310773   |   9.11   \n",
      "   5    |     870     |    598.647949   |   9.12   \n",
      "   5    |     880     |    598.659979   |   9.12   \n",
      "   5    |     890     |    598.205035   |   9.12   \n",
      "   5    |     900     |    598.898077   |   9.11   \n",
      "   5    |     910     |    597.983179   |   9.11   \n",
      "   5    |     920     |    598.799139   |   9.11   \n",
      "   5    |     930     |    598.708148   |   9.12   \n",
      "   5    |     940     |    598.840753   |   9.12   \n",
      "   5    |     950     |    598.833691   |   9.12   \n",
      "   5    |     960     |    598.255688   |   9.12   \n",
      "   5    |     970     |    598.228967   |   9.11   \n",
      "   5    |     980     |    598.335260   |   9.11   \n",
      "   5    |     990     |    598.993158   |   9.12   \n",
      "   5    |    1000     |    599.315137   |   9.12   \n",
      "   5    |    1010     |    598.386993   |   9.11   \n",
      "   5    |    1020     |    598.697137   |   9.12   \n",
      "   5    |    1030     |    599.115424   |   9.12   \n",
      "   5    |    1040     |    598.620520   |   9.11   \n",
      "   5    |    1050     |    599.056897   |   9.12   \n",
      "   5    |    1060     |    599.384125   |   9.11   \n",
      "   5    |    1070     |    598.057782   |   9.12   \n",
      "   5    |    1080     |    599.114764   |   9.11   \n",
      "   5    |    1090     |    598.289514   |   9.11   \n",
      "   5    |    1100     |    599.254675   |   9.11   \n",
      "   5    |    1110     |    598.780865   |   9.11   \n",
      "   5    |    1120     |    598.617609   |   9.11   \n",
      "   5    |    1125     |    531.180579   |   4.14   \n",
      "   5    |     -      |   598.448264   |   4.14   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  | 每10个Batch  |    训练集 Loss    |   时间    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "   6    |     10      |    597.776811   |   10.03  \n",
      "   6    |     20      |    597.787585   |   9.11   \n",
      "   6    |     30      |    598.074750   |   9.11   \n",
      "   6    |     40      |    597.468958   |   9.14   \n",
      "   6    |     50      |    597.608649   |   9.11   \n",
      "   6    |     60      |    597.526367   |   9.11   \n",
      "   6    |     70      |    597.950348   |   9.11   \n",
      "   6    |     80      |    597.425806   |   9.11   \n",
      "   6    |     90      |    596.630988   |   9.11   \n",
      "   6    |     100     |    597.854663   |   9.10   \n",
      "   6    |     110     |    597.704242   |   9.12   \n",
      "   6    |     120     |    597.430707   |   9.10   \n",
      "   6    |     130     |    597.291827   |   9.11   \n",
      "   6    |     140     |    598.139740   |   9.11   \n",
      "   6    |     150     |    598.275830   |   9.12   \n",
      "   6    |     160     |    597.087299   |   9.12   \n",
      "   6    |     170     |    597.260992   |   9.16   \n",
      "   6    |     180     |    597.000421   |   9.12   \n",
      "   6    |     190     |    597.524634   |   9.11   \n",
      "   6    |     200     |    597.795361   |   9.11   \n",
      "   6    |     210     |    597.204382   |   9.11   \n",
      "   6    |     220     |    597.868170   |   9.12   \n",
      "   6    |     230     |    597.643109   |   9.11   \n",
      "   6    |     240     |    597.096216   |   9.11   \n",
      "   6    |     250     |    597.655914   |   9.11   \n",
      "   6    |     260     |    596.930292   |   9.11   \n",
      "   6    |     270     |    597.590302   |   9.11   \n",
      "   6    |     280     |    597.079633   |   9.11   \n",
      "   6    |     290     |    597.916754   |   9.12   \n",
      "   6    |     300     |    597.305658   |   9.11   \n",
      "   6    |     310     |    598.544855   |   9.12   \n",
      "   6    |     320     |    597.626324   |   9.11   \n",
      "   6    |     330     |    597.016327   |   9.11   \n",
      "   6    |     340     |    596.773193   |   9.11   \n",
      "   6    |     350     |    597.677307   |   9.11   \n",
      "   6    |     360     |    597.366351   |   9.12   \n",
      "   6    |     370     |    598.065796   |   9.12   \n",
      "   6    |     380     |    598.110089   |   9.12   \n",
      "   6    |     390     |    597.562872   |   9.12   \n",
      "   6    |     400     |    598.725269   |   9.11   \n",
      "   6    |     410     |    597.456543   |   9.12   \n",
      "   6    |     420     |    597.451648   |   9.12   \n",
      "   6    |     430     |    597.184314   |   9.12   \n",
      "   6    |     440     |    597.956372   |   9.12   \n",
      "   6    |     450     |    597.271191   |   9.12   \n",
      "   6    |     460     |    597.938794   |   9.12   \n",
      "   6    |     470     |    597.803485   |   9.11   \n",
      "   6    |     480     |    597.100104   |   9.12   \n",
      "   6    |     490     |    596.959937   |   9.18   \n",
      "   6    |     500     |    598.037799   |   9.11   \n",
      "   6    |     510     |    597.432593   |   9.12   \n",
      "   6    |     520     |    596.766418   |   9.11   \n",
      "   6    |     530     |    597.518640   |   9.11   \n",
      "   6    |     540     |    597.718335   |   9.11   \n",
      "   6    |     550     |    597.351093   |   9.12   \n",
      "   6    |     560     |    597.385980   |   9.11   \n",
      "   6    |     570     |    598.474561   |   9.12   \n",
      "   6    |     580     |    597.065521   |   9.12   \n",
      "   6    |     590     |    597.378571   |   9.11   \n",
      "   6    |     600     |    596.634552   |   9.11   \n",
      "   6    |     610     |    597.754102   |   9.11   \n",
      "   6    |     620     |    597.511932   |   9.12   \n",
      "   6    |     630     |    597.060828   |   9.12   \n",
      "   6    |     640     |    597.849835   |   9.12   \n",
      "   6    |     650     |    597.026465   |   9.12   \n",
      "   6    |     660     |    597.827173   |   9.17   \n",
      "   6    |     670     |    597.397522   |   9.12   \n",
      "   6    |     680     |    596.577130   |   9.12   \n",
      "   6    |     690     |    597.589587   |   9.12   \n",
      "   6    |     700     |    597.029260   |   9.12   \n",
      "   6    |     710     |    597.036810   |   9.12   \n",
      "   6    |     720     |    597.140369   |   9.11   \n",
      "   6    |     730     |    597.565833   |   9.12   \n",
      "   6    |     740     |    597.443866   |   9.11   \n",
      "   6    |     750     |    597.063782   |   9.12   \n",
      "   6    |     760     |    597.676935   |   9.11   \n",
      "   6    |     770     |    596.832068   |   9.11   \n",
      "   6    |     780     |    597.040009   |   9.11   \n",
      "   6    |     790     |    597.011383   |   9.12   \n",
      "   6    |     800     |    596.763623   |   9.12   \n",
      "   6    |     810     |    597.707159   |   9.11   \n",
      "   6    |     820     |    597.165985   |   9.12   \n",
      "   6    |     830     |    597.339795   |   9.11   \n",
      "   6    |     840     |    597.416388   |   9.11   \n",
      "   6    |     850     |    596.970831   |   9.11   \n",
      "   6    |     860     |    597.608606   |   9.12   \n",
      "   6    |     870     |    597.765302   |   9.12   \n",
      "   6    |     880     |    596.894098   |   9.12   \n",
      "   6    |     890     |    597.884644   |   9.12   \n",
      "   6    |     900     |    597.183960   |   9.11   \n",
      "   6    |     910     |    598.051794   |   9.12   \n",
      "   6    |     920     |    597.151520   |   9.12   \n",
      "   6    |     930     |    596.905878   |   9.13   \n",
      "   6    |     940     |    596.174878   |   9.11   \n",
      "   6    |     950     |    596.854663   |   9.12   \n",
      "   6    |     960     |    596.818848   |   9.11   \n",
      "   6    |     970     |    596.991650   |   9.12   \n",
      "   6    |     980     |    598.010449   |   9.16   \n",
      "   6    |     990     |    597.692981   |   9.11   \n",
      "   6    |    1000     |    597.219342   |   9.12   \n",
      "   6    |    1010     |    597.028442   |   9.12   \n",
      "   6    |    1020     |    596.825830   |   9.13   \n",
      "   6    |    1030     |    597.084589   |   9.13   \n",
      "   6    |    1040     |    597.379443   |   9.12   \n",
      "   6    |    1050     |    597.176862   |   9.11   \n",
      "   6    |    1060     |    597.223291   |   9.11   \n",
      "   6    |    1070     |    597.942047   |   9.12   \n",
      "   6    |    1080     |    597.135529   |   9.11   \n",
      "   6    |    1090     |    597.167780   |   9.12   \n",
      "   6    |    1100     |    597.700049   |   9.11   \n",
      "   6    |    1110     |    597.324670   |   9.11   \n",
      "   6    |    1120     |    597.012927   |   9.11   \n",
      "   6    |    1125     |    529.960693   |   4.14   \n",
      "   6    |     -      |   597.113925   |   4.14   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  | 每10个Batch  |    训练集 Loss    |   时间    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "   7    |     10      |    595.789118   |   10.03  \n",
      "   7    |     20      |    596.788275   |   9.17   \n",
      "   7    |     30      |    596.704224   |   9.11   \n",
      "   7    |     40      |    596.026324   |   9.99   \n",
      "   7    |     50      |    597.255023   |   9.11   \n",
      "   7    |     60      |    596.287366   |   9.12   \n",
      "   7    |     70      |    596.644690   |   9.11   \n",
      "   7    |     80      |    596.517810   |   9.11   \n",
      "   7    |     90      |    596.416168   |   9.12   \n",
      "   7    |     100     |    596.907660   |   9.11   \n",
      "   7    |     110     |    596.773535   |   9.12   \n",
      "   7    |     120     |    596.715033   |   9.11   \n",
      "   7    |     130     |    596.756946   |   9.12   \n",
      "   7    |     140     |    596.232806   |   9.11   \n",
      "   7    |     150     |    596.318750   |   9.11   \n",
      "   7    |     160     |    596.687732   |   9.12   \n",
      "   7    |     170     |    596.221619   |   9.12   \n",
      "   7    |     180     |    596.565277   |   9.11   \n",
      "   7    |     190     |    596.857007   |   9.11   \n",
      "   7    |     200     |    595.917548   |   9.12   \n",
      "   7    |     210     |    596.323663   |   9.11   \n",
      "   7    |     220     |    596.609137   |   9.12   \n",
      "   7    |     230     |    596.987384   |   9.12   \n",
      "   7    |     240     |    597.221216   |   9.12   \n",
      "   7    |     250     |    596.797754   |   9.11   \n",
      "   7    |     260     |    596.160480   |   9.11   \n",
      "   7    |     270     |    596.522443   |   9.11   \n",
      "   7    |     280     |    596.851593   |   9.11   \n",
      "   7    |     290     |    596.317633   |   9.11   \n",
      "   7    |     300     |    596.181714   |   9.11   \n",
      "   7    |     310     |    595.876886   |   9.11   \n",
      "   7    |     320     |    596.578247   |   9.11   \n",
      "   7    |     330     |    596.625787   |   9.12   \n",
      "   7    |     340     |    596.543976   |   9.17   \n",
      "   7    |     350     |    596.365204   |   9.12   \n",
      "   7    |     360     |    596.651019   |   9.12   \n",
      "   7    |     370     |    596.716412   |   9.12   \n",
      "   7    |     380     |    596.636578   |   9.12   \n",
      "   7    |     390     |    596.284198   |   9.12   \n",
      "   7    |     400     |    596.375781   |   9.12   \n",
      "   7    |     410     |    595.562805   |   9.12   \n",
      "   7    |     420     |    596.316339   |   9.13   \n",
      "   7    |     430     |    596.401501   |   9.12   \n",
      "   7    |     440     |    596.110101   |   9.12   \n",
      "   7    |     450     |    596.080994   |   9.12   \n",
      "   7    |     460     |    595.815924   |   9.12   \n",
      "   7    |     470     |    596.553192   |   9.12   \n",
      "   7    |     480     |    596.473572   |   9.13   \n",
      "   7    |     490     |    596.573419   |   9.13   \n",
      "   7    |     500     |    595.855945   |   9.13   \n",
      "   7    |     510     |    596.490594   |   9.19   \n",
      "   7    |     520     |    595.822595   |   9.13   \n",
      "   7    |     530     |    595.941443   |   9.13   \n",
      "   7    |     540     |    596.342920   |   9.13   \n",
      "   7    |     550     |    596.074829   |   9.14   \n",
      "   7    |     560     |    596.502155   |   9.14   \n",
      "   7    |     570     |    596.442865   |   9.14   \n",
      "   7    |     580     |    596.419470   |   9.13   \n",
      "   7    |     590     |    596.053412   |   9.12   \n",
      "   7    |     600     |    596.195367   |   9.13   \n",
      "   7    |     610     |    596.604010   |   9.13   \n",
      "   7    |     620     |    596.554681   |   9.14   \n",
      "   7    |     630     |    596.557458   |   9.13   \n",
      "   7    |     640     |    595.948700   |   9.13   \n",
      "   7    |     650     |    596.582953   |   9.13   \n",
      "   7    |     660     |    596.228229   |   9.13   \n",
      "   7    |     670     |    596.077423   |   9.14   \n",
      "   7    |     680     |    596.342590   |   9.13   \n",
      "   7    |     690     |    595.525024   |   9.13   \n",
      "   7    |     700     |    595.787177   |   9.13   \n",
      "   7    |     710     |    596.541901   |   9.13   \n",
      "   7    |     720     |    596.615021   |   9.14   \n",
      "   7    |     730     |    595.698730   |   9.13   \n",
      "   7    |     740     |    596.042065   |   9.13   \n",
      "   7    |     750     |    595.387659   |   9.14   \n",
      "   7    |     760     |    596.297485   |   9.14   \n",
      "   7    |     770     |    597.023816   |   9.13   \n",
      "   7    |     780     |    596.782928   |   9.14   \n",
      "   7    |     790     |    596.568036   |   9.13   \n",
      "   7    |     800     |    596.261707   |   9.14   \n",
      "   7    |     810     |    596.848163   |   9.13   \n",
      "   7    |     820     |    596.620526   |   9.19   \n",
      "   7    |     830     |    595.953119   |   9.13   \n",
      "   7    |     840     |    596.273999   |   9.13   \n",
      "   7    |     850     |    595.732196   |   9.14   \n",
      "   7    |     860     |    596.084186   |   9.13   \n",
      "   7    |     870     |    597.056995   |   9.14   \n",
      "   7    |     880     |    595.710699   |   9.14   \n",
      "   7    |     890     |    596.447510   |   9.14   \n",
      "   7    |     900     |    596.201532   |   9.14   \n",
      "   7    |     910     |    596.274988   |   9.13   \n",
      "   7    |     920     |    596.380713   |   9.13   \n",
      "   7    |     930     |    596.143768   |   9.14   \n",
      "   7    |     940     |    596.684509   |   9.13   \n",
      "   7    |     950     |    596.403949   |   9.13   \n",
      "   7    |     960     |    596.238892   |   9.14   \n",
      "   7    |     970     |    596.082129   |   9.13   \n",
      "   7    |     980     |    596.333667   |   9.13   \n",
      "   7    |     990     |    596.593378   |   9.18   \n",
      "   7    |    1000     |    596.578918   |   9.14   \n",
      "   7    |    1010     |    596.877332   |   9.13   \n",
      "   7    |    1020     |    595.601447   |   9.14   \n",
      "   7    |    1030     |    596.431293   |   9.13   \n",
      "   7    |    1040     |    595.767584   |   9.14   \n",
      "   7    |    1050     |    595.756836   |   9.13   \n",
      "   7    |    1060     |    596.296362   |   9.13   \n",
      "   7    |    1070     |    596.208075   |   9.13   \n",
      "   7    |    1080     |    596.754108   |   9.13   \n",
      "   7    |    1090     |    595.388788   |   9.13   \n",
      "   7    |    1100     |    596.297748   |   9.13   \n",
      "   7    |    1110     |    596.043658   |   9.13   \n",
      "   7    |    1120     |    595.720294   |   9.13   \n",
      "   7    |    1125     |    529.000708   |   4.15   \n",
      "   7    |     -      |   596.042040   |   4.15   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  | 每10个Batch  |    训练集 Loss    |   时间    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "   8    |     10      |    595.369912   |   10.05  \n",
      "   8    |     20      |    595.111664   |   9.12   \n",
      "   8    |     30      |    595.346838   |   9.13   \n",
      "   8    |     40      |    595.326648   |   9.26   \n",
      "   8    |     50      |    595.505695   |   9.13   \n",
      "   8    |     60      |    595.115076   |   9.13   \n",
      "   8    |     70      |    595.824152   |   9.13   \n",
      "   8    |     80      |    595.645331   |   9.14   \n",
      "   8    |     90      |    595.661176   |   9.14   \n",
      "   8    |     100     |    595.820868   |   9.13   \n",
      "   8    |     110     |    595.525098   |   9.13   \n",
      "   8    |     120     |    595.196509   |   9.13   \n",
      "   8    |     130     |    594.982544   |   9.13   \n",
      "   8    |     140     |    595.535400   |   9.13   \n",
      "   8    |     150     |    594.990375   |   9.13   \n",
      "   8    |     160     |    595.702795   |   9.13   \n",
      "   8    |     170     |    595.508270   |   9.13   \n",
      "   8    |     180     |    595.069250   |   9.19   \n",
      "   8    |     190     |    595.720044   |   9.13   \n",
      "   8    |     200     |    595.399927   |   9.14   \n",
      "   8    |     210     |    595.543756   |   9.13   \n",
      "   8    |     220     |    595.052490   |   9.13   \n",
      "   8    |     230     |    595.855103   |   9.13   \n",
      "   8    |     240     |    595.346857   |   9.13   \n",
      "   8    |     250     |    595.801971   |   9.13   \n",
      "   8    |     260     |    595.155389   |   9.13   \n",
      "   8    |     270     |    595.028418   |   9.13   \n",
      "   8    |     280     |    595.043353   |   9.13   \n",
      "   8    |     290     |    595.307190   |   9.13   \n",
      "   8    |     300     |    595.683295   |   9.13   \n",
      "   8    |     310     |    596.221967   |   9.13   \n",
      "   8    |     320     |    595.457709   |   9.13   \n",
      "   8    |     330     |    595.429181   |   9.13   \n",
      "   8    |     340     |    595.371271   |   9.14   \n",
      "   8    |     350     |    595.934814   |   9.18   \n",
      "   8    |     360     |    595.168884   |   9.14   \n",
      "   8    |     370     |    596.057959   |   9.13   \n",
      "   8    |     380     |    595.068231   |   9.13   \n",
      "   8    |     390     |    595.246808   |   9.13   \n",
      "   8    |     400     |    595.262433   |   9.14   \n",
      "   8    |     410     |    595.229633   |   9.14   \n",
      "   8    |     420     |    595.058154   |   9.14   \n",
      "   8    |     430     |    595.648035   |   9.13   \n",
      "   8    |     440     |    595.518262   |   9.13   \n",
      "   8    |     450     |    595.217871   |   9.13   \n",
      "   8    |     460     |    595.660944   |   9.14   \n",
      "   8    |     470     |    595.058551   |   9.13   \n",
      "   8    |     480     |    594.916406   |   9.13   \n",
      "   8    |     490     |    594.873676   |   9.13   \n",
      "   8    |     500     |    595.207477   |   9.12   \n",
      "   8    |     510     |    595.796313   |   9.14   \n",
      "   8    |     520     |    595.004797   |   9.13   \n",
      "   8    |     530     |    595.736456   |   9.13   \n",
      "   8    |     540     |    595.185785   |   9.14   \n",
      "   8    |     550     |    594.890430   |   9.13   \n",
      "   8    |     560     |    595.571960   |   9.13   \n",
      "   8    |     570     |    595.693060   |   9.13   \n",
      "   8    |     580     |    595.087482   |   9.14   \n",
      "   8    |     590     |    595.435132   |   9.13   \n",
      "   8    |     600     |    595.360510   |   9.13   \n",
      "   8    |     610     |    594.722852   |   9.13   \n",
      "   8    |     620     |    595.252911   |   9.13   \n",
      "   8    |     630     |    595.944135   |   9.13   \n",
      "   8    |     640     |    595.116931   |   9.13   \n",
      "   8    |     650     |    595.335925   |   9.13   \n",
      "   8    |     660     |    595.050897   |   9.13   \n",
      "   8    |     670     |    595.671869   |   9.19   \n",
      "   8    |     680     |    595.503973   |   9.13   \n",
      "   8    |     690     |    595.461670   |   9.14   \n",
      "   8    |     700     |    595.691559   |   9.13   \n",
      "   8    |     710     |    595.210663   |   9.13   \n",
      "   8    |     720     |    595.108984   |   9.13   \n",
      "   8    |     730     |    595.665466   |   9.14   \n",
      "   8    |     740     |    595.482263   |   9.14   \n",
      "   8    |     750     |    595.134229   |   9.14   \n",
      "   8    |     760     |    595.821826   |   9.14   \n",
      "   8    |     770     |    595.285211   |   9.13   \n",
      "   8    |     780     |    595.312000   |   9.14   \n",
      "   8    |     790     |    595.852289   |   9.13   \n",
      "   8    |     800     |    595.614233   |   9.14   \n",
      "   8    |     810     |    595.370740   |   9.13   \n",
      "   8    |     820     |    595.501740   |   9.14   \n",
      "   8    |     830     |    595.448175   |   9.13   \n",
      "   8    |     840     |    595.555438   |   9.19   \n",
      "   8    |     850     |    595.004565   |   9.14   \n",
      "   8    |     860     |    595.277185   |   9.13   \n",
      "   8    |     870     |    595.877557   |   9.14   \n",
      "   8    |     880     |    594.633234   |   9.13   \n",
      "   8    |     890     |    595.513379   |   9.14   \n",
      "   8    |     900     |    594.994971   |   9.13   \n",
      "   8    |     910     |    594.703802   |   9.13   \n",
      "   8    |     920     |    594.858740   |   9.14   \n",
      "   8    |     930     |    595.361523   |   9.14   \n",
      "   8    |     940     |    595.046057   |   9.13   \n",
      "   8    |     950     |    595.517413   |   9.13   \n",
      "   8    |     960     |    595.057111   |   9.13   \n",
      "   8    |     970     |    595.596075   |   9.13   \n",
      "   8    |     980     |    595.180652   |   9.14   \n",
      "   8    |     990     |    595.480829   |   9.13   \n",
      "   8    |    1000     |    595.306702   |   9.13   \n",
      "   8    |    1010     |    594.589856   |   9.14   \n",
      "   8    |    1020     |    595.033832   |   9.14   \n",
      "   8    |    1030     |    594.612799   |   9.13   \n",
      "   8    |    1040     |    594.904675   |   9.13   \n",
      "   8    |    1050     |    595.753345   |   9.13   \n",
      "   8    |    1060     |    595.105457   |   9.13   \n",
      "   8    |    1070     |    595.610095   |   9.15   \n",
      "   8    |    1080     |    595.266754   |   9.13   \n",
      "   8    |    1090     |    595.293719   |   9.14   \n",
      "   8    |    1100     |    595.369092   |   9.13   \n",
      "   8    |    1110     |    595.764972   |   9.13   \n",
      "   8    |    1120     |    595.004034   |   9.13   \n",
      "   8    |    1125     |    527.877136   |   4.15   \n",
      "   8    |     -      |   595.052003   |   4.15   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  | 每10个Batch  |    训练集 Loss    |   时间    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "   9    |     10      |    595.693359   |   10.05  \n",
      "   9    |     20      |    595.132117   |   9.13   \n",
      "   9    |     30      |    594.685382   |   9.18   \n",
      "   9    |     40      |    594.382434   |   9.15   \n",
      "   9    |     50      |    594.712048   |   9.13   \n",
      "   9    |     60      |    595.023120   |   9.13   \n",
      "   9    |     70      |    594.709131   |   9.13   \n",
      "   9    |     80      |    594.844897   |   9.13   \n",
      "   9    |     90      |    594.190436   |   9.13   \n",
      "   9    |     100     |    594.638916   |   9.12   \n",
      "   9    |     110     |    595.189276   |   9.13   \n",
      "   9    |     120     |    594.300391   |   9.13   \n",
      "   9    |     130     |    594.637201   |   9.13   \n",
      "   9    |     140     |    594.696088   |   9.13   \n",
      "   9    |     150     |    594.745197   |   9.13   \n",
      "   9    |     160     |    593.969006   |   9.13   \n",
      "   9    |     170     |    594.671631   |   9.13   \n",
      "   9    |     180     |    594.496069   |   9.13   \n",
      "   9    |     190     |    595.130896   |   9.13   \n",
      "   9    |     200     |    594.656635   |   9.19   \n",
      "   9    |     210     |    594.302765   |   9.13   \n",
      "   9    |     220     |    594.693140   |   9.14   \n",
      "   9    |     230     |    594.338013   |   9.13   \n",
      "   9    |     240     |    595.355414   |   9.13   \n",
      "   9    |     250     |    594.766248   |   9.13   \n",
      "   9    |     260     |    595.297675   |   9.13   \n",
      "   9    |     270     |    594.421332   |   9.14   \n",
      "   9    |     280     |    595.058441   |   9.14   \n",
      "   9    |     290     |    595.238092   |   9.13   \n",
      "   9    |     300     |    594.751245   |   9.13   \n",
      "   9    |     310     |    594.806689   |   9.14   \n",
      "   9    |     320     |    594.999915   |   9.13   \n",
      "   9    |     330     |    595.041711   |   9.13   \n",
      "   9    |     340     |    594.662372   |   9.13   \n",
      "   9    |     350     |    594.652679   |   9.14   \n",
      "   9    |     360     |    594.377704   |   9.14   \n",
      "   9    |     370     |    593.873596   |   9.13   \n",
      "   9    |     380     |    594.916827   |   9.13   \n",
      "   9    |     390     |    594.640045   |   9.13   \n",
      "   9    |     400     |    594.128625   |   9.14   \n",
      "   9    |     410     |    594.804413   |   9.13   \n",
      "   9    |     420     |    595.068555   |   9.13   \n",
      "   9    |     430     |    594.480151   |   9.13   \n",
      "   9    |     440     |    595.091589   |   9.13   \n",
      "   9    |     450     |    594.638855   |   9.13   \n",
      "   9    |     460     |    594.187885   |   9.14   \n",
      "   9    |     470     |    594.704169   |   9.14   \n",
      "   9    |     480     |    594.459491   |   9.13   \n",
      "   9    |     490     |    594.664374   |   9.13   \n",
      "   9    |     500     |    595.250244   |   9.13   \n",
      "   9    |     510     |    595.064417   |   9.13   \n",
      "   9    |     520     |    594.368738   |   9.19   \n",
      "   9    |     530     |    594.132617   |   9.13   \n",
      "   9    |     540     |    594.889630   |   9.13   \n",
      "   9    |     550     |    594.554260   |   9.14   \n",
      "   9    |     560     |    594.067444   |   9.13   \n",
      "   9    |     570     |    594.324054   |   9.13   \n",
      "   9    |     580     |    594.442310   |   9.13   \n",
      "   9    |     590     |    594.657172   |   9.13   \n",
      "   9    |     600     |    594.736841   |   9.14   \n",
      "   9    |     610     |    594.832245   |   9.14   \n",
      "   9    |     620     |    594.515717   |   9.14   \n",
      "   9    |     630     |    594.784680   |   9.13   \n",
      "   9    |     640     |    594.466724   |   9.14   \n",
      "   9    |     650     |    595.087103   |   9.13   \n",
      "   9    |     660     |    594.676093   |   9.13   \n",
      "   9    |     670     |    595.020062   |   9.14   \n",
      "   9    |     680     |    594.796234   |   9.18   \n",
      "   9    |     690     |    594.554889   |   9.13   \n",
      "   9    |     700     |    594.651013   |   9.13   \n",
      "   9    |     710     |    594.644086   |   9.14   \n",
      "   9    |     720     |    595.232770   |   9.13   \n",
      "   9    |     730     |    594.542902   |   9.14   \n",
      "   9    |     740     |    594.180054   |   9.13   \n",
      "   9    |     750     |    594.442303   |   9.13   \n",
      "   9    |     760     |    594.468097   |   9.13   \n",
      "   9    |     770     |    594.752118   |   9.13   \n",
      "   9    |     780     |    594.680261   |   9.14   \n",
      "   9    |     790     |    594.534216   |   9.13   \n",
      "   9    |     800     |    593.933228   |   9.14   \n",
      "   9    |     810     |    594.656506   |   9.13   \n",
      "   9    |     820     |    594.775854   |   9.13   \n",
      "   9    |     830     |    594.356750   |   9.13   \n",
      "   9    |     840     |    594.302185   |   9.13   \n",
      "   9    |     850     |    594.294794   |   9.13   \n",
      "   9    |     860     |    594.942145   |   9.13   \n",
      "   9    |     870     |    594.708356   |   9.14   \n",
      "   9    |     880     |    594.492493   |   9.13   \n",
      "   9    |     890     |    594.381305   |   9.14   \n",
      "   9    |     900     |    594.562085   |   9.13   \n",
      "   9    |     910     |    594.757495   |   9.14   \n",
      "   9    |     920     |    593.796387   |   9.13   \n",
      "   9    |     930     |    594.928223   |   9.14   \n",
      "   9    |     940     |    594.481958   |   9.13   \n",
      "   9    |     950     |    594.829993   |   9.13   \n",
      "   9    |     960     |    594.370587   |   9.13   \n",
      "   9    |     970     |    593.850647   |   9.13   \n",
      "   9    |     980     |    594.527454   |   9.13   \n",
      "   9    |     990     |    594.141797   |   9.13   \n",
      "   9    |    1000     |    594.512567   |   9.19   \n",
      "   9    |    1010     |    594.759143   |   9.13   \n",
      "   9    |    1020     |    593.823639   |   9.14   \n",
      "   9    |    1030     |    594.437384   |   9.13   \n",
      "   9    |    1040     |    594.243079   |   9.13   \n",
      "   9    |    1050     |    594.824738   |   9.14   \n",
      "   9    |    1060     |    593.908466   |   9.13   \n",
      "   9    |    1070     |    593.827094   |   9.14   \n",
      "   9    |    1080     |    594.489807   |   9.13   \n",
      "   9    |    1090     |    594.628864   |   9.13   \n",
      "   9    |    1100     |    594.496967   |   9.13   \n",
      "   9    |    1110     |    594.470380   |   9.14   \n",
      "   9    |    1120     |    594.611725   |   9.13   \n",
      "   9    |    1125     |    527.946130   |   4.15   \n",
      "   9    |     -      |   594.315649   |   4.15   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  | 每10个Batch  |    训练集 Loss    |   时间    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "  10    |     10      |    594.155068   |   10.04  \n",
      "  10    |     20      |    594.129419   |   9.14   \n",
      "  10    |     30      |    593.927173   |   9.13   \n",
      "  10    |     40      |    593.870966   |   9.20   \n",
      "  10    |     50      |    593.858728   |   9.13   \n",
      "  10    |     60      |    594.820886   |   9.13   \n",
      "  10    |     70      |    594.124884   |   9.13   \n",
      "  10    |     80      |    594.377637   |   9.13   \n",
      "  10    |     90      |    593.867957   |   9.13   \n",
      "  10    |     100     |    593.969836   |   9.12   \n",
      "  10    |     110     |    594.321704   |   9.13   \n",
      "  10    |     120     |    595.098029   |   9.13   \n",
      "  10    |     130     |    594.144550   |   9.14   \n",
      "  10    |     140     |    594.285974   |   9.13   \n",
      "  10    |     150     |    594.412042   |   9.13   \n",
      "  10    |     160     |    594.050696   |   9.14   \n",
      "  10    |     170     |    593.961615   |   9.13   \n",
      "  10    |     180     |    593.771490   |   9.13   \n",
      "  10    |     190     |    593.967920   |   9.13   \n",
      "  10    |     200     |    593.560114   |   9.13   \n",
      "  10    |     210     |    594.012335   |   9.12   \n",
      "  10    |     220     |    594.250128   |   9.13   \n",
      "  10    |     230     |    593.914978   |   9.13   \n",
      "  10    |     240     |    594.046948   |   9.13   \n",
      "  10    |     250     |    594.062677   |   9.13   \n",
      "  10    |     260     |    593.820813   |   9.13   \n",
      "  10    |     270     |    594.070660   |   9.13   \n",
      "  10    |     280     |    594.312915   |   9.12   \n",
      "  10    |     290     |    593.911115   |   9.13   \n",
      "  10    |     300     |    593.528040   |   9.13   \n",
      "  10    |     310     |    593.808936   |   9.13   \n",
      "  10    |     320     |    593.525476   |   9.13   \n",
      "  10    |     330     |    594.508112   |   9.13   \n",
      "  10    |     340     |    594.145599   |   9.13   \n",
      "  10    |     350     |    594.137091   |   9.13   \n",
      "  10    |     360     |    593.815887   |   9.18   \n",
      "  10    |     370     |    593.842151   |   9.13   \n",
      "  10    |     380     |    594.164813   |   9.13   \n",
      "  10    |     390     |    594.348413   |   9.13   \n",
      "  10    |     400     |    594.623706   |   9.13   \n",
      "  10    |     410     |    595.025201   |   9.13   \n",
      "  10    |     420     |    594.382269   |   9.13   \n",
      "  10    |     430     |    593.445245   |   9.13   \n",
      "  10    |     440     |    594.216248   |   9.13   \n",
      "  10    |     450     |    594.027899   |   9.13   \n",
      "  10    |     460     |    593.807684   |   9.14   \n",
      "  10    |     470     |    593.813446   |   9.14   \n",
      "  10    |     480     |    594.979095   |   9.13   \n",
      "  10    |     490     |    594.336572   |   9.14   \n",
      "  10    |     500     |    593.955103   |   9.13   \n",
      "  10    |     510     |    594.255157   |   9.14   \n",
      "  10    |     520     |    594.098737   |   9.13   \n",
      "  10    |     530     |    594.077069   |   9.19   \n",
      "  10    |     540     |    593.975708   |   9.13   \n",
      "  10    |     550     |    593.648926   |   9.13   \n",
      "  10    |     560     |    593.578491   |   9.13   \n",
      "  10    |     570     |    593.552795   |   9.13   \n",
      "  10    |     580     |    594.571674   |   9.14   \n",
      "  10    |     590     |    594.146515   |   9.13   \n",
      "  10    |     600     |    594.263770   |   9.13   \n",
      "  10    |     610     |    593.848499   |   9.13   \n",
      "  10    |     620     |    594.204474   |   9.14   \n",
      "  10    |     630     |    594.419177   |   9.13   \n",
      "  10    |     640     |    594.192944   |   9.14   \n",
      "  10    |     650     |    594.457410   |   9.14   \n",
      "  10    |     660     |    593.945959   |   9.13   \n",
      "  10    |     670     |    594.240057   |   9.13   \n",
      "  10    |     680     |    594.541229   |   9.13   \n",
      "  10    |     690     |    594.227631   |   9.14   \n",
      "  10    |     700     |    594.784485   |   9.13   \n",
      "  10    |     710     |    594.288861   |   9.14   \n",
      "  10    |     720     |    594.217517   |   9.13   \n",
      "  10    |     730     |    594.429126   |   9.13   \n",
      "  10    |     740     |    593.654120   |   9.14   \n",
      "  10    |     750     |    594.352740   |   9.13   \n",
      "  10    |     760     |    594.293414   |   9.13   \n",
      "  10    |     770     |    594.488251   |   9.13   \n",
      "  10    |     780     |    594.450024   |   9.14   \n",
      "  10    |     790     |    593.999683   |   9.14   \n",
      "  10    |     800     |    593.993726   |   9.14   \n",
      "  10    |     810     |    594.613574   |   9.13   \n",
      "  10    |     820     |    594.300488   |   9.14   \n",
      "  10    |     830     |    594.280273   |   9.13   \n",
      "  10    |     840     |    594.013019   |   9.14   \n",
      "  10    |     850     |    594.162311   |   9.19   \n",
      "  10    |     860     |    594.015607   |   9.13   \n",
      "  10    |     870     |    594.397687   |   9.13   \n",
      "  10    |     880     |    594.240564   |   9.13   \n",
      "  10    |     890     |    593.667474   |   9.13   \n",
      "  10    |     900     |    594.150592   |   9.13   \n",
      "  10    |     910     |    594.006012   |   9.14   \n",
      "  10    |     920     |    593.997186   |   9.13   \n",
      "  10    |     930     |    594.489484   |   9.13   \n",
      "  10    |     940     |    594.218506   |   9.14   \n",
      "  10    |     950     |    593.809473   |   9.14   \n",
      "  10    |     960     |    593.737054   |   9.13   \n",
      "  10    |     970     |    594.391718   |   9.14   \n",
      "  10    |     980     |    594.058435   |   9.14   \n",
      "  10    |     990     |    594.014185   |   9.13   \n",
      "  10    |    1000     |    593.826233   |   9.13   \n",
      "  10    |    1010     |    594.161249   |   9.13   \n",
      "  10    |    1020     |    594.076074   |   9.19   \n",
      "  10    |    1030     |    594.147284   |   9.13   \n",
      "  10    |    1040     |    593.903931   |   9.14   \n",
      "  10    |    1050     |    593.824530   |   9.13   \n",
      "  10    |    1060     |    594.380066   |   9.13   \n",
      "  10    |    1070     |    593.905768   |   9.14   \n",
      "  10    |    1080     |    593.752594   |   9.13   \n",
      "  10    |    1090     |    594.079425   |   9.13   \n",
      "  10    |    1100     |    594.278290   |   9.13   \n",
      "  10    |    1110     |    593.734601   |   9.14   \n",
      "  10    |    1120     |    593.978503   |   9.14   \n",
      "  10    |    1125     |    527.124719   |   4.15   \n",
      "  10    |     -      |   593.821532   |   4.15   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_model, optimizer, scheduler = initialize_model(epochs=10)\n",
    "# print(\"Start training and validation:\\n\")\n",
    "print(\"Start training and testing:\\n\")\n",
    "train(cross_model, train_dataloader, val_dataloader, epochs=10, step1=True, step2=False, step3=False)  # 这个是有评估的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde3cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b57f352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cross_model, 'cross_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb4f3e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef047c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f679408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4000, dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a= torch.tensor(([1,2,3,1,5]),dtype=float)\n",
    "a.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2a3f147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(100)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(torch.tensor(-10),torch.tensor(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32a85576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b369594c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor(1), tensor(2)), (tensor(1), tensor(3)), (tensor(2), tensor(3))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.combinations(torch.tensor([1,2,3]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe72c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa84669b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66227427",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_text_inputs, b_text_masks, b_text_attribute_label, b_text_word_together, \\\n",
    "b_img_obj_feat, b_img_obj_bbox, b_img_ocr_feat, b_img_ocr_bbox, b_img_masks, b_editor_distant = train_dataloader.__iter__().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59b4a631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 61])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_text_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adf2414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_text_inputs = b_text_inputs.to(device)\n",
    "b_text_masks = b_text_masks.to(device)\n",
    "b_text_attribute_label = b_text_attribute_label.to(device)\n",
    "b_text_word_together = b_text_word_together.to(device)\n",
    "b_img_obj_feat = b_img_obj_feat.to(device)\n",
    "b_img_obj_bbox = b_img_obj_bbox.to(device)\n",
    "b_img_ocr_feat = b_img_ocr_feat.to(device)\n",
    "b_img_ocr_bbox = b_img_ocr_bbox.to(device)\n",
    "b_img_masks = b_img_masks.to(device)\n",
    "b_editor_distant = b_editor_distant.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf0b27ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "cross_model, optimizer, scheduler = initialize_model(epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce187350",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cls, text_last_hidden_state, img_cls, img_last_hidden_state \\\n",
    "= cross_model(b_text_inputs, b_text_masks, b_img_obj_feat, b_img_obj_bbox, b_img_ocr_feat, b_img_ocr_bbox, b_img_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98b5b1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2775, -0.1659, -0.0778,  ..., -0.1822,  0.1579,  0.4985],\n",
       "        [-0.8105, -0.4942, -0.5276,  ..., -0.0192,  0.4456, -0.0930],\n",
       "        [-0.3033,  0.0124, -0.3964,  ...,  0.0385,  0.0782,  0.2377],\n",
       "        ...,\n",
       "        [ 0.1432, -0.0670, -0.0317,  ..., -0.1520,  0.1794,  0.3405],\n",
       "        [ 0.0601,  0.4484, -0.1519,  ..., -0.1184,  0.0894, -0.0289],\n",
       "        [-0.0931,  0.3288, -0.5555,  ...,  0.1626,  0.4895, -0.0292]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc2d02be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6549,  0.2449,  0.0435,  ...,  0.8504,  0.1459, -1.2015],\n",
       "        [ 0.6749, -0.2373, -1.2552,  ...,  0.0999,  0.1404, -0.8349],\n",
       "        [ 0.8559,  0.0077, -0.7344,  ...,  0.4500,  0.2473, -0.9800],\n",
       "        ...,\n",
       "        [ 0.4493,  0.3524, -1.2906,  ...,  0.7226,  0.7867, -1.1003],\n",
       "        [ 0.4568,  0.2141, -0.7252,  ...,  0.9375,  0.4080, -1.3505],\n",
       "        [ 0.0148,  0.6836, -0.1288,  ...,  1.3798,  0.3511, -1.6702]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1119db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = NCE_loss(text_cls,img_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "401be27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(709.8785, device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6146eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/CDistNet/lib/python3.7/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    }
   ],
   "source": [
    "img_cls.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16b2730e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_model.img_cls_Linear.bias.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b70b8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_model.img_cls_Linear.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b485860",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88406ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.7289e-11, -1.7608e-09, -1.0696e-09, -7.2760e-12, -4.5225e-10,\n",
       "        -2.9331e-10,  1.4370e-10, -1.4588e-09,  8.7311e-11,  9.1222e-10,\n",
       "        -7.6398e-10,  1.7872e-10, -5.5138e-12,  9.0608e-11, -4.2928e-10,\n",
       "        -8.5493e-11, -1.5643e-10,  7.7080e-10,  1.1596e-10, -9.3678e-10,\n",
       "        -8.9494e-10, -2.5011e-10, -4.2928e-10, -4.6293e-10,  2.0736e-10,\n",
       "         4.6857e-09, -2.5011e-11,  4.1837e-10,  6.9488e-10, -4.5839e-10,\n",
       "        -1.4552e-09, -2.1646e-10,  3.7471e-10,  2.3574e-09,  7.6852e-11,\n",
       "         8.1855e-11, -1.5666e-10,  1.5716e-09, -2.0555e-10, -2.0373e-10,\n",
       "         3.7835e-10,  2.5102e-10, -6.8076e-10, -7.2396e-10, -4.3224e-10,\n",
       "         5.5479e-11,  1.7644e-09,  1.2415e-10,  2.7012e-10,  1.6712e-10,\n",
       "        -9.7680e-10,  2.2628e-09,  2.5193e-10, -2.4966e-10, -6.5847e-10,\n",
       "         5.7298e-10,  3.5561e-10,  1.0043e-09, -9.8225e-11, -3.7102e-10,\n",
       "        -5.3853e-10, -2.7940e-09,  1.5916e-11, -1.4290e-10, -3.5216e-09,\n",
       "        -9.9317e-10, -1.0459e-10,  9.3223e-10, -2.2555e-10,  2.8284e-10,\n",
       "        -5.7923e-11,  4.2837e-10, -1.6826e-10, -6.2414e-11, -4.2746e-11,\n",
       "         1.3233e-10, -7.6398e-11, -7.8393e-10, -3.0377e-10, -3.6380e-10,\n",
       "         3.1832e-10, -5.3296e-10, -6.8371e-10,  1.3642e-10, -4.4748e-10,\n",
       "         4.8894e-09,  2.6193e-09, -3.2514e-11,  3.2196e-10,  2.0452e-10,\n",
       "        -1.4785e-10,  1.4916e-10, -8.6766e-10,  5.6752e-10,  4.6930e-10,\n",
       "         5.2751e-10,  6.6939e-10, -1.8872e-11,  6.6166e-11, -2.5739e-10,\n",
       "         7.8580e-10, -3.9790e-11,  1.5075e-10, -7.9581e-11,  3.0122e-09,\n",
       "         5.8208e-11,  3.8210e-10,  2.7647e-10, -2.6034e-10, -1.2642e-10,\n",
       "        -7.4942e-10,  2.8449e-09,  4.0018e-11,  1.4506e-10, -1.3097e-09,\n",
       "         2.3238e-10, -4.1655e-10,  7.8671e-11,  1.8713e-10, -7.6852e-10,\n",
       "        -3.1832e-10, -6.8417e-10,  6.3665e-11, -9.9575e-11, -1.4870e-10,\n",
       "         5.6752e-10, -1.0459e-11,  1.3160e-09, -2.5032e-09,  6.1119e-10,\n",
       "         2.0736e-09, -4.4565e-10, -6.0027e-10,  1.2506e-10, -2.3192e-10,\n",
       "        -1.0341e-09, -1.4552e-09,  1.4029e-10, -1.1223e-09, -1.7062e-09,\n",
       "         4.6348e-09, -1.9611e-10, -5.6389e-10, -8.5493e-11,  1.2424e-09,\n",
       "         4.2974e-11,  1.8626e-09,  2.4284e-10,  9.6043e-10, -4.0473e-11,\n",
       "        -3.2742e-11,  8.0036e-11, -1.1505e-10, -1.5831e-10, -1.9418e-10,\n",
       "        -4.2837e-10, -6.7553e-10, -1.9627e-09, -1.4252e-09,  9.7316e-11,\n",
       "        -2.8376e-10, -6.4858e-11, -5.5934e-11, -1.3151e-09,  9.9271e-10,\n",
       "         1.2813e-10, -1.6898e-09,  3.1378e-11, -7.2760e-10, -3.1832e-12,\n",
       "         1.8190e-11, -5.0932e-11, -2.3874e-11,  6.3665e-10,  5.1841e-11,\n",
       "         2.0736e-10, -4.3201e-11, -4.3428e-10,  6.5484e-10, -2.0256e-08,\n",
       "        -2.8240e-10,  2.0555e-10, -5.2341e-10,  3.0013e-09, -2.9695e-10,\n",
       "         1.3461e-10, -5.7912e-10,  3.6925e-10, -5.5465e-11,  1.5643e-10,\n",
       "         1.9395e-10,  2.0020e-10, -5.6889e-10, -2.9831e-09, -8.5493e-11,\n",
       "        -7.5306e-10, -2.8831e-10, -1.4370e-10,  6.1118e-10,  2.2737e-10,\n",
       "        -1.7448e-08,  1.5575e-10,  1.8190e-10, -2.2055e-11, -2.1669e-10,\n",
       "         2.6375e-11,  5.1841e-11,  3.5106e-10,  1.7826e-10,  1.1187e-10,\n",
       "        -5.5479e-10,  7.8785e-11, -9.9681e-10, -5.0022e-11, -4.7294e-09,\n",
       "        -7.2077e-10,  3.3034e-09,  4.5839e-10,  2.5466e-10, -6.8212e-11,\n",
       "        -6.1050e-10, -5.9117e-11, -6.9758e-10,  5.0022e-11,  5.0613e-10,\n",
       "         2.4374e-09,  2.1055e-10, -6.1846e-10,  5.2478e-10, -5.0750e-10,\n",
       "        -1.0823e-10, -1.3324e-10,  3.6380e-10, -2.2646e-10,  3.5288e-10,\n",
       "         1.4325e-11,  3.4674e-10, -7.4579e-11, -4.9909e-11, -9.5497e-12,\n",
       "        -7.0827e-11, -4.2292e-10, -6.9105e-10,  2.9286e-10,  1.0277e-10,\n",
       "         1.3824e-10,  1.3165e-10, -1.0914e-10,  7.2760e-11, -3.4447e-10,\n",
       "        -2.6830e-11,  6.0572e-10,  1.3358e-10, -3.2560e-10, -4.9795e-10,\n",
       "        -2.4411e-09,  3.0195e-10, -1.6774e-10,  6.2573e-10,  5.5297e-10,\n",
       "         4.6566e-10,  1.9668e-10,  1.4984e-10, -1.1532e-09,  1.7894e-09,\n",
       "        -5.3660e-11, -7.3851e-10,  4.9567e-10,  3.1605e-10,  1.0873e-09,\n",
       "         2.0191e-10, -2.0862e-09,  1.8235e-10,  2.2419e-10,  1.3097e-10,\n",
       "        -1.7435e-09,  1.0687e-11, -4.0745e-10,  4.8203e-11,  4.8385e-10,\n",
       "        -1.7826e-09, -2.1600e-11,  1.4752e-09, -2.5466e-11, -1.6416e-10,\n",
       "         1.9827e-10,  9.8953e-10, -1.8576e-10, -1.2486e-08,  5.4570e-12,\n",
       "         5.1568e-10,  6.2573e-10,  8.3219e-11, -1.1369e-10, -5.3251e-10,\n",
       "        -9.1677e-10,  1.7599e-10, -1.3824e-10, -1.9099e-11, -4.3201e-12,\n",
       "         3.2742e-11,  2.9468e-10,  1.4279e-10,  3.8017e-10,  1.8647e-10,\n",
       "        -1.7735e-11, -2.7740e-10, -4.4747e-10,  1.1642e-10,  1.8917e-10,\n",
       "         3.8199e-11,  2.5830e-10,  5.0932e-10,  3.2219e-10, -2.6921e-10,\n",
       "         1.0118e-10, -1.7553e-10,  2.9831e-10,  8.2309e-11, -9.6861e-11,\n",
       "        -1.3893e-10, -4.5475e-12, -6.1846e-11,  1.5098e-10, -2.3356e-09,\n",
       "         1.2387e-09,  1.0361e-08, -1.8736e-10, -5.1841e-10, -1.0914e-11,\n",
       "        -5.3842e-10,  1.9782e-11, -3.1014e-10,  1.5916e-10, -1.8781e-10,\n",
       "        -1.8554e-10, -1.0295e-09, -1.2278e-10, -2.7467e-10,  4.8703e-10,\n",
       "         7.7853e-10,  5.4799e-11, -1.1642e-10,  3.3033e-09, -1.7790e-09,\n",
       "        -6.7485e-10, -2.7194e-10, -3.0923e-10,  3.8199e-11, -6.4938e-10,\n",
       "        -5.1932e-10, -1.8190e-09,  4.9113e-11,  2.1907e-10, -7.9854e-10,\n",
       "        -1.1187e-10,  4.2064e-11, -5.6502e-11,  1.4143e-10, -4.3337e-10,\n",
       "        -2.5611e-09, -2.7126e-10, -3.9872e-09,  2.8376e-10,  1.7681e-09,\n",
       "         1.2420e-10,  8.5129e-10,  7.2760e-11, -4.5475e-10, -6.0027e-11,\n",
       "         1.9736e-10, -9.6770e-10,  1.5250e-08,  1.2922e-08,  9.0586e-10,\n",
       "         1.9209e-09,  6.5484e-11,  3.3528e-08, -9.7134e-10, -6.7521e-09,\n",
       "        -7.1850e-11, -4.2246e-10,  4.5111e-10, -1.6098e-10,  7.0941e-11,\n",
       "        -2.9104e-10, -2.1379e-10,  3.2869e-09,  2.2337e-09, -1.8986e-10,\n",
       "        -3.7471e-10, -4.2701e-10,  4.7294e-11, -1.8044e-09, -8.7766e-11,\n",
       "        -4.2178e-11, -3.6380e-09,  1.4188e-10,  4.2974e-10,  1.7462e-10,\n",
       "         1.1460e-10,  4.9477e-10,  4.6384e-11,  7.5033e-12,  3.6380e-10,\n",
       "        -1.6189e-10,  2.3323e-10, -1.4988e-09, -1.2369e-10,  5.5297e-10,\n",
       "        -1.7503e-09,  3.2765e-10, -3.4379e-10, -1.7985e-10,  2.0077e-09,\n",
       "        -5.8799e-10,  2.4556e-10,  1.4438e-11,  4.7294e-10,  5.5688e-09,\n",
       "         1.0913e-09,  2.0646e-10,  2.2010e-10, -1.0914e-11, -2.8667e-09,\n",
       "         3.8654e-10, -2.7940e-09,  3.7335e-10,  5.0522e-10,  1.9863e-09,\n",
       "         1.2096e-10, -3.7653e-10,  4.9113e-11, -1.4734e-10,  3.8469e-10,\n",
       "        -1.4606e-09, -4.6020e-10, -2.0918e-10,  3.2014e-10,  2.3374e-10,\n",
       "        -8.0036e-11,  6.8394e-10, -8.9517e-10,  1.1187e-10, -2.0191e-10,\n",
       "         1.8190e-11,  1.6356e-08,  5.7298e-11, -2.3135e-10,  3.3037e-10,\n",
       "        -7.2760e-12,  4.0384e-10,  5.0932e-10, -5.6843e-10,  2.1475e-10,\n",
       "         2.2465e-10,  4.0454e-09,  4.6566e-10,  8.0149e-12,  4.7282e-10,\n",
       "        -1.5461e-10,  2.8555e-10, -5.9481e-10,  1.8827e-10,  1.5461e-11,\n",
       "        -7.6830e-10,  1.4370e-10,  1.0068e-09,  2.5102e-09, -5.2694e-11,\n",
       "        -3.4152e-10, -4.0200e-10,  1.6134e-09, -5.1455e-10,  8.4128e-10,\n",
       "        -1.4552e-11, -1.4137e-10,  2.0577e-10, -1.2960e-10, -3.2742e-11,\n",
       "        -4.5402e-09,  2.2737e-11,  1.4552e-11,  1.3388e-09,  5.7753e-11,\n",
       "         9.3678e-11, -1.7371e-10,  1.3597e-10, -2.8813e-09,  9.3223e-10,\n",
       "        -1.1823e-11,  2.0509e-10,  5.7753e-11, -2.8103e-10, -4.2559e-10,\n",
       "        -3.0786e-10, -1.6416e-10,  5.4570e-12, -2.1391e-09, -2.6012e-10,\n",
       "         1.3233e-10, -2.4680e-08,  7.6398e-11, -1.6939e-10,  9.0267e-11,\n",
       "        -5.4570e-11, -2.2628e-09,  5.0932e-10,  3.3447e-10,  2.6557e-10,\n",
       "         5.0022e-11,  4.0745e-10,  2.1760e-10, -6.5484e-11,  1.1687e-10,\n",
       "         2.6921e-10,  5.9572e-11,  2.6512e-10,  1.0004e-11,  9.8566e-11,\n",
       "         5.9954e-09, -4.5475e-11,  6.5484e-10,  5.7753e-10,  2.1828e-10,\n",
       "        -3.0923e-11,  1.2806e-09,  7.8160e-12,  4.2382e-10,  3.3197e-10,\n",
       "        -2.5784e-10, -1.0114e-09,  1.3188e-11,  2.0518e-09, -3.7880e-10,\n",
       "         1.1569e-09,  2.3147e-10,  3.1723e-09,  3.8381e-10,  3.2196e-10,\n",
       "        -1.7099e-10, -2.5102e-10,  1.0186e-10, -5.8935e-10,  1.7280e-10,\n",
       "        -6.1618e-11, -4.3656e-10, -6.4574e-11,  2.2169e-10,  1.0539e-10,\n",
       "        -2.1100e-10, -4.3912e-11, -4.8385e-10,  5.3478e-10,  2.6284e-10,\n",
       "         6.8667e-11,  1.9463e-10,  3.0195e-10,  5.8571e-10, -4.7203e-10,\n",
       "        -3.8142e-11, -1.5666e-10,  1.9668e-10,  3.4051e-09, -6.1518e-09,\n",
       "         8.5856e-10,  1.0185e-10,  2.0736e-10,  1.6007e-10,  1.6189e-10,\n",
       "         9.4587e-11,  1.2460e-10, -1.0195e-09,  3.0650e-09,  1.5962e-10,\n",
       "         8.1855e-12, -1.1041e-09, -1.2915e-10,  2.7103e-10, -1.8281e-10,\n",
       "        -8.3219e-11,  8.5493e-11,  6.9895e-10, -9.4587e-10,  1.8135e-09,\n",
       "        -9.1404e-11, -3.0923e-11, -5.3433e-11, -7.7853e-10,  4.3656e-10,\n",
       "         5.0568e-10, -2.5261e-10,  2.4102e-11,  6.8565e-10, -2.6421e-10,\n",
       "        -1.8827e-10, -9.4587e-11, -3.1650e-10, -2.4556e-10, -1.3824e-09,\n",
       "         5.4570e-10, -3.2514e-11,  2.7539e-09, -3.1150e-11,  5.9139e-08,\n",
       "         1.0914e-10,  2.6193e-10, -5.8208e-11,  1.2187e-10, -7.6170e-11,\n",
       "         2.7649e-10,  1.6007e-10,  1.1897e-10,  2.8285e-10,  1.6780e-10,\n",
       "         3.1537e-10, -9.3678e-11,  2.6193e-10,  1.8053e-10,  5.9117e-10,\n",
       "         2.3101e-10,  1.2742e-09, -3.7232e-11, -1.5098e-10,  9.0495e-11,\n",
       "         1.0871e-11, -1.8372e-10,  1.6917e-10,  2.5057e-10, -2.4247e-09,\n",
       "         7.4884e-08, -5.5297e-09,  9.1859e-11,  4.5497e-10,  4.4429e-10,\n",
       "        -2.2283e-10,  3.3288e-10,  4.3656e-11, -3.6380e-11, -7.6352e-10,\n",
       "         1.0878e-09,  6.8212e-12, -3.1605e-11, -2.9468e-10,  3.0518e-05,\n",
       "         3.1287e-10,  2.6560e-10, -2.9905e-09,  3.6516e-09,  1.7826e-10,\n",
       "        -3.2514e-11,  1.0596e-10,  1.1937e-10,  2.2737e-11,  5.0932e-11,\n",
       "        -7.9126e-11, -3.1753e-10,  2.2192e-10, -2.3283e-10,  5.5843e-10,\n",
       "         1.1405e-09, -3.6789e-10,  1.6225e-09,  5.0022e-11, -4.5820e-09,\n",
       "         1.5052e-10,  2.1419e-10,  4.7658e-10,  1.6735e-10, -4.2201e-10,\n",
       "        -1.7371e-10, -1.9572e-09,  1.3039e-08, -1.3824e-10,  7.1395e-10,\n",
       "        -6.6211e-10, -2.4011e-10, -4.2292e-11,  2.2703e-10, -6.9929e-10,\n",
       "        -2.0100e-10, -1.7826e-10, -9.8044e-10,  1.9099e-11,  8.5129e-10,\n",
       "         1.8626e-09,  1.1896e-09, -3.4197e-10,  1.5689e-11,  2.9104e-10,\n",
       "         2.2374e-10, -2.6193e-10, -9.6406e-11,  5.3660e-11, -4.9113e-10,\n",
       "        -7.1850e-11,  7.0713e-11,  7.9581e-11, -8.4856e-10, -2.6375e-10,\n",
       "        -2.7853e-10, -1.4552e-09, -7.6398e-11,  1.3688e-10, -1.1187e-10,\n",
       "        -1.7317e-09,  2.3738e-10, -4.7294e-11,  3.1832e-12,  1.6307e-09,\n",
       "        -3.5470e-11,  7.0008e-10, -1.5170e-09,  1.0823e-09,  1.4961e-10,\n",
       "         1.7608e-09,  2.5611e-09,  1.5971e-09, -2.0145e-10, -2.6830e-11,\n",
       "         3.2014e-10, -1.1860e-09, -1.2562e-11, -5.2154e-08, -1.7099e-10,\n",
       "         1.8213e-10,  4.7294e-10,  3.8926e-10, -2.2078e-10, -1.2878e-09,\n",
       "         2.9217e-10, -1.8099e-10, -1.4072e-09,  2.4284e-10, -2.9905e-10,\n",
       "        -5.3296e-09, -2.0714e-10, -1.4370e-10,  2.7008e-08, -2.6330e-10,\n",
       "        -2.5329e-10, -5.2410e-11, -1.4944e-10,  1.8690e-10, -4.0473e-10,\n",
       "         7.5437e-08, -1.1300e-10,  1.3097e-10, -4.8312e-09, -5.1587e-09,\n",
       "         2.0441e-10,  6.0595e-11,  0.0000e+00, -7.1304e-10, -4.5247e-10,\n",
       "        -1.6789e-09,  3.9108e-11,  6.0936e-11, -2.9195e-10, -8.2764e-11,\n",
       "         1.6280e-10,  5.2978e-10, -8.8463e-11,  1.7371e-10,  2.9704e-09,\n",
       "         1.4297e-09,  3.2232e-09,  9.7589e-10], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_model.img_cls_Linear.bias.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "43aff85ed666934b379dabfd1f9e215dacf1069bbc74c049ec83f24ccabc3074"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
